Linux is case sensitive language
RHCSA - EX200
RHEL8 			vs		RHEL7
released 2019				2014
XFS					XFS
max XFS size 1024 TiB			max XFS size 512 TiB
max RAM supports 24TiB			max RAM supports 12TiB
supports podman for containarizaton	by default supports docker


Linux			vs		Unix
Linus dev Linux				denies richie and ken thomsan
by modifying unix kernal		dev from scratch for motorola, HP etc
					was not possible to use at home due to high memory and RAM size
flavor:					linux, MacOS, SunSolaris etc derived from Unix
fedora: Redhat and CentOS(free)
debian: ubunto and MintOS, Kali

Highly secured + multiple layer of security + stable + supports previous hardware version + does not run virus +
supports all file system + Open source + used from smart watch to super computer.

cat /etc/redhat-release

~ - shows home directory
#: root user
$: shows normal user
pwd - shows present working dir
ls -alh -> shows .(current dir) and ..(parent dir) + so cd .. -- will move you to parent dir    h-option shows size in human redable format like kb,mb,gb etc instead of bytes
ls -d : list all sub-directories

<permision> <link> <user> <group> <size> <DATE and TIME modified><File>
user permissions first char shows d<dir> or -<normal file> or s<socket file> or l<link> etc
second 3 chars shows <owner> permission
next 3 char shows <group> permisison 
next 3 shows <other>
example: dr-xr-xr-x 		read-4 write-2 execute-1

mkdir -p -> -p options used to create parent child dir
rm command always comes with promt + alias command shows linux commands with promt for exmaple: alias rm ---> shows rm='rm -i' here -i asks for input
if you dont want command to promt then unalias <command name>

sftp is used same as scp but secured one. sftp username@server --> change dir to file location --> mget filename (to download file) or we can use ? to see sftp options


Day1-7:
touch: command used to create empty file + manipulate file timestamp (touch -t yymmddhhmm(custom date which you want) <filename>)
more: will show file content per page and press v to enter in vi mode. 
file <filename> : cat command can't open binary files so use file command to check type of file. 
strings <binary filename>: to cat binary file

Day8: shell expansion: shell checks the command + manipulate it and then pass it to kernal
'': echo as it is
"": echo value written inside ""
<<: use as custom marker: to come out of file while writing. example: cat > hello.txt <<stop --> if i write hello,hi,stop  then will come out of cat command. and hello.txt file will only have <hello,hi,>


control operator: used to execute multiple commands 
; : used to execute mutiple commnads serially one after other. example: date;cal;echo $SHELL
& : execute cmnds parallely (execute cmnd in background)
? : its variable which stores result of last command.  stores value = 0 if last cmnd success else any random value
&& : logical AND (also used to execute multiple commands in sequence having dependancy on previous task)
||: logical OR
#: shell ignores the cmmands written after #
\ : whatever operator you give post \, shell will ignore that. 


shell variables:
1. system defined var:capital letters defined by Developers. (SHELL,PWD,HOSTNAME,USER etc) 
2. user defined var: user defined. 
If you want to permanently use env variables then define in .bashrc file 

set: shows env varibales in system
unset <var> : to unset env variable
echo $PS1 : PS1 is shell env variable shows how the perticular shell prompt looks like. [sh shell shows [u@host ~]# or $]. we can change shell prompt using PS1 variable
like PS1 = 'Hello Shantayya' then aftr logging to shell you will see [hello Shantayya]:cursor
PATH: var used by shell to check where the camnds binaries are. 
SHLVL: used to see level of shell. [example: if you executed zsh inside bash shell then you will on 2nd level] we can use shell inside shell
env -i: used to ignore env variables exported by user.

Shell embadding, history and options:
`` backticks: works as $()
!!: bang sign to execute last command
history -c: to clear history + we can repeat perticular command with !<number from hitory> + HISTSIZE shows how many commands history can store + >.bash_history to permanently clear history
export HISTTIMEFORMAT='%d %m %y %T': used to set date and time for history commands + history -d <line number> : to delete perticular history command
space cmnd: comands does not goto history but works only in debian os.

diable_bash_history.sh
#!/bin/bash
if [$# -eq 0];
then
echo "usage: $0 username"
exit 1
fi

su -c "echo 'export HISTIGNORE=\"&\"' >> /home/$1/.bashrc" - $1
Note:  & typically represents the process ID of the last background command executed in the shell. So, if you execute this command (without "" in &), the value of HISTIGNORE will be set to the process ID of the last background command, not the literal & character.

chmod 744 disable_bash_history.sh
./disable_bash_history.sh username


File Globbing: used to dynamically generate file names: [ex: ls *file*]
? - one char
+ - one or more char
* - zero or more char
[] - char between []
[!]- no char between[]
use LANG=C env var to see case sansitive output if ls [A-Z]ile command shows output having small letter file,file123 etc

Day13: Input and output redirection 
stdin (signal 0): standered input through whch we send input to shell (keyboard)
stdout(1>): through which shell transfer output to user (display/file). cmand > output.txt
stderr(2>): error output (display/file)		cmnd 2>error.txt

noclobber: avoid to overrite file using (set -o noclobber --> it will add C in noclobber output). if you try to overrite existing file, it throws error.
forcefully overrite: then use commnd >| filename
(output redirection) 2>&1 : if we are not sure cmnd gives output or error then we use 2>&1 to redirect out+error in single file. 
/dev/null 2>&1: used to stop cluttering (show output on screen) then we redirect output to blackhole. yum install -y httpd >/dev/null 2>&1 + >filename: used to nullify
(input redirection) <: example: cat <filename    


Day14: Filters used in Linux
commands which are used with | called filters. 
tac filename: used to reverse content. (tee also works like cat)
grep: used to search perticular string in files. -i: ignore case -v: except string  -A1/2/3: lines after string  -B1/2/3: print lines before string  -w if want to fetch one result out of many given results.

passwd file has 7 columns: <user><encryption passwd><user id><grp id><user display name><home dir><default shell>

cut: used to show columns from result. -d:delimiter -f:fields -c:char
ex: cut -d: -f1,3 /etc/passwd 

tr: used to translate content   (ex: cat say.txt | tr 's' 'S' --> it will translate all small s to S)
wc: -l:line -w:words -c:char
sort: as name suggest   -k1/2/3: 1st/2nd/3rd col (ex: sort -k2 say.txt)
uniq: use with sort -c: used to count occurenece of words
comm: used to see multiple files common entries together. (ex: comm file1 file2)
sed: used to manipulate file content -i:change in file directly, d: delete line with string  s: change old to new string (ex: sed 's/old/new/' or sed '/string/d')

time cmnd: used to check how much time taken by command to execute
tar tvf: used to see files inside tar           c:create v:verbose p:prserve permission x:untar f:file z:gz ext

find <location>  -type f/d -perm <perm to search on /a:r/w/x> -iname <filename> -empty<to see empty files> -user<search with owner> -group -mtime <+/-days> -mmin <+/- min> -atime<access time in days> -size<+/- M/G>
if we have to execute other operation after search the use -exec cmnd {} \;  (ex: find . -type f -perm 0777 -print -exec chmod 0555 {} \;)

locate: very powerful command to search faster. <use updatedb command to update index>

Day16: reguler expression
BRE: basic regex-> use -G with grep	--> grep -G 's\|ya' say.txt        ya$--> end of string with ya 	^ya--> starts with
ERE: extended regex --> use -E with grep  --> ex: grep -E 's|ya' say.txt
PRCE: perl regex --> use -P with grep

\bstring\b or -w: search with space before and after


Day17: stream editor
sed: used to manipulate file content 
-i:change in file directly, d: delete line with string  s: change old to new string (ex: sed 's/old/new/' or sed '/string/d') 
echo Sunday | sed 's_\(Sun\)_1ny_' --> it will replace Sun with Sunny--> Sunnyday
echo 2014-10-01 \; if we need to replace - with + then: echo 2014-10-01 | sed 's/\(....\)-\(..\)-\(..\)/\1+\2+\3/' 
cat filename | sed 's/o\{2,3\}/A/' --> it will replace oo or ooo with A



Day18: Introduction to vi editor
vi: is old editor however vim provides extra features with lot of flexibility. its modified vi editor. 
1. command mode -> default mode, press escape (create,open etc)
2. insert mode: when we need to edit file. press -> insert key 
aAiIoO: these will work in command mode 
i: start writing from current char	I: start writing from start of current line
a: aftr current char	A: end of current line
o: after current line	O: before current line
x: delete current char   X: delete previous char
u : undo changes	.: redo changes
r <new char>: to replace current char with new one
p: paste after current line 	P: paste before current line		dd: cut current line	yy: copy current line 
to copy or cut multiple lines use number and dd or yy then use p or P
$: to jump to last char of line		0: to jump to start of line		d$: delete current char to last char of line.	d0: delete current char to start char of line
G: starting page 	gg: last line of page
J: to merge previous and current line,<cursor should be at start position before pressing J)
ddp: switch current and next line
w: word by word next jump	b: backward word to word jump		dw: delete word
/string: forward search		?string: backward search

:r cmdn or filename: used to add comamd result or new file content in current file

buffers: in vi editor 36 types of buffers are being used. 
"add: to cut current line and add to buffer		"ap: paste buffered line

vim file1 file2 file3: To modify multiple files	:args to see which file we are updating		:n to move to next file		:rew to move back to previous file
:e toggle to previous edited file

:set number or se nu --> shows line numbers	create .vimrc file with set number options to open all file with line number 

vimtutor: very powerful tool to learn vim editor.
copy only saves last copied line where as in buffere we can save upto 36 copies. 

set -x #debug mode
set -e #Exit on error
set -o #Exit on pipefail


Day19: Local user management   [user entry get added to /etc/shadow, /etc/group /etc/passwd file]
who: whoever has logged in to machine
w: whoever is executing what command and when logged in to machine.
id: shows user id
su user: switching to user account temporarely (pretending to be user). it will not take you to home directory. 		
su - user: completely switch to user account with pwd as home dir. using env variables of that user. 

sudo: super user do
visudo : when one update [user	ALL:ALL		NOPASSWD: ALL] then perticular user perform all commands without any password promt. if we passwd promt then remove NOPASSWD from line.
user	ALL:ALL		NOPASSWD: ALL, !/usr/sbin/useradd	[limit command execution: user cann't execute useradd command]	
if we want to limit admin permisions user wise, we can put all users in group and update sudoer permission for that group.

0(root)-1000: id are preserve for system. linux allocated user id after 1000 to normal users.
passwd file has 7 columns: <user><encryption passwd stored in /etc/shadow file><user id><grp id><user display name or desciption><home dir><default shell>
<default shell>: we can define /sbin/nologin/ to make the account as service acocunt so user can't login

ideal way to add user: useradd -m <modify> -d <home dir default perm 700> -c <display name> -s <shell> username
Here we have created user with only homedir + display name but /etc/passwd contains default shell,uid,gid etc so that information stored in /etc/default/useradd or useradd -D to check default entries
we can modify these setting

userdel -r username: -r deletes home dir as well
usermod -s <shell>: we can modify default shell	or chsh -s /bin/sh username
chsh -l: shows shell available in system
chown user:group dir: to change dir ownership		chmod 700 dir

usermod --no-home-create --shell /bin/false user	--> create user with no home directory and with /bin/false shell (service accounts) system user

sudo useradd -r -s /sbin/nologin apache			--> -r create system user
system users need: Instead of running the Apache service under a generic user account, you create a dedicated system user for Apache:

/etc/skel: this is very important dir which contains profile files. and whenever new user is created then these files are copied to new user home dir.
you can push new features to new user accounts through this dir files.



Day20: password management    Note: linux uses crypt function at backgnd for passwrd encryption
sudo su -: if user entry is there in sudoer file but you dont know root passwrd then also you can su to root using. if promt for passwd provide user passwrd.
passwd user: root to reset user passwd for user.

password related settings contained in /etc/shadow/:
/etc/shadow: file contains <user> <encrypted passwd> <no.of days since pass not chnaged calculate from 1Jan1970> <after how many days passwd shld chnaged><passwd expiry days> <warning to be thrown no of days before expry> <disable no. of days>
/etc/login.defs: contains default password settings (min_pass_days,max_pass_days,warn_pass_days etc) related info. admin can chnage these setting so that it will apply to all the users. 

chage username: to modify default passwd setting for perticular user. 
chage -l username: to list paswd settings for perticuler user. 

openssl passwd <password>: it will give diff encryp pass all the time. we can use -salt datastring to define paswd . openssl passwd -salt <number> <password>

useradd -m -p $(openssl passwd <password>) username --> it will create user with encrypted passwd. but issue with this command is password will appear in history.
lock user: usermod -L username	(this will add ! in /etc/shadow) 	or 	passwd -l username (this will add !! in /etc/shadow)
unlock user: usermod -U username 	or 	passwd -u username


Day21: profile files
1. system profile files: works globally : each shell (bash/ksh) source the profile setting(PATH, PS1,HOSTNAME, execute /etc/inputrc(keyboard flags), /etc/bashrc etc) from /etc/profile to current env or you can put file in /etc/profile.d directory
2. user profile files: works for perticular user

.bash_profile: used to add $HOME to PATH and source .bashrc file + this file loads at every login. 
.bashrc : executes this file at every shell open + source global /etc/bashrc file + user spefic functions and env vars to save permanetly
.bash_logout: tells system that user has logged out. 



Day22: Group management 
if want to provide permisisons on mass level then create group and add all users to that group, provide reqrd permsision to group.

primary group: whenever user added to system at the same time same name group is also created. each user is part of its primary group. /etc/group 
secondary group: groups apart from primary are called secondary 

groups: shows user is part of which groups
usermod -a<append> -g<add primary gr> -G<add secondry grp> UNIX<group name> unix<usr name>: add group to existing user
vi /etc/group 			: 		add user with , separated to existing group.
groupmod -n <new name> <old name>: 		change existing group name.
groupdel grpname		: 		delet grp but it should not be primary
gpasswd -A <username> <grpname>	: 		To make user as Admin of the group  --> gpasswd -a <usrname> <grpname> : Admin can add user to group 
Note: its not necessary that admin to be a part of group. 

gpasswd -d <username> <grpname> : 		delete user from grp
gpasswd -A "" <grpname> 	: 		to dismiss all admins from group



Day23: File security in Linux
File security is managed through 
1. standard permissons
2. advanced permissions

ls -lih --> 
<inode> <file type + permisison of file on own,grp,othr,SGID or SUID  + sticky bit + ACL symbol> <links> <owner> <grp> <size> <date> <file>
file permission:
1. char represent type of file (d:dir,-:normal file,l:links b:block s:socket etc)
2. next 9 char shows owner,group and other permissions

r: 4	w: 2	x: 1 --> total 7(Full perm)	0(no perm)

Schenario1: Shantayya wants read access on file1
solution: 
1. change onwer to shantayya if file1 only read by him. 	[chown <onwer> filename]	or [chgrp <grpname> filename]
2. add him to group and assign necessary perm on group		[chmod 600 filename]
3. provide perm to others so he can read. but here other users also read then which is security breach		[chmod 666 filename]

If we want to remove execute permission from user,group and other then
chmod u-x filename; chmod g-x filename; chmod o-x filename;				Note: use u+x,g+x,o+x to add execute permission to user,grp and others 

Note: based on the umask 4 digit number default permissions are assigned to new created files. (umask :  to see what number is defined for umask)
1st digit: advance perm		2nd: user	3rd: group		4th: other
umask number is defined in profile files. 
max default permission on any file=666<user><grp><other>		--> system does not give default execute perm due to security.
(max perm)  - UMASK  = effective permission on new files
  666	    - 022    = 644 (rw-r--r--)

max default permission on any directory = 777<user><grp><other>		--> without execute permission user can't access dir + make sure with execute we give read access otherwise its of no use. 
(max perm)  - UMASK  = effective permission on new directories
  777	    - 022    = 755 (rwxr-xr-xr)

mkdir -m <perm> dirname 	 	: used to create dir with permission. 
cp -rp <dir or file> <destination>	: preserve permisisons while copying

Note: even if script does not have execute perm then one can run using sh shell. ideally we should use ./ beacuse script written using bash shell.


Day24: advanced file security
umask -S		: shows perm symbolically
umask -S u=rwx,go=	: set umask perm symbolically

sticky bit: shows that user can only delete files owned by him in directory.  
chmod o+t dirname	: to apply sticky bit		or append 1 while executing chmod --> chmod 1777 <dirname>
chmod o-t dirname	: to remove stiky bit

If other user has execute permisison then sticky bit will be t (small case) otherwise T

SGID	: set group id bit --> shows group of all files under dir is same group of that dir when we create any new file. we append 2 while applying perm on dir 
chmod 2777 dirname  or chmod g+s dirname	: to apply SGID bit
chmod 777 dirname  or chmod g-s dirname		: to remove SGID bit

SUID	: set user id bit	--> It executes any file as its owner or root is executing. 
chmod 4777 filename  or chmod u+s filename	: to apply SUID bit
chmod 00777 filename  or chmod u-s filename  	: to remove SUID bit

Note: even if we read special and advanced permissions but these are not enough in situations where diff users need dif permissions. ther we use access control list.


Day25: Access control list (shows + at the end of permissions otherwise shows .)
cat /etc/fstab		: defaults shows that fs supports ACL.
getfacl filename	: shows permissions available on file
setfacl -m --no-mask u:<usernam>:<perm>	filename	: provid acl permission to user on file		if dont want to update effective mask value then use --no-mask in command
setfacl -m g:<grpname>:<perm>	filename	: provide acl perm on group
setfacl -b filename				: remove acl from file
setfacl -x g:<grpname> <filename>		: to remove acl from group
Note: mask value is any max perm assigned 


Day26: Inodes,Hard and soft links 
Note: RHL8 xfs dont have option to disable acl. this option was there in ext4 fs.
Inode		: ds which stores file metadata except file name and content.
hard link	: ln filename link_name		+create separate file with same content 	+link not increases	+deleting orig file has no impact on hard link
soft link	: ln -s filename link_name	+create shortcut				+link increases		+deleing orig file makes file not accessible


Day27: Process Management
Process: a compiled source or prgm running in system. each process has unique ID.
Parent process ID: always less than child ID.
system always start init(earlier linux versions) or systemd(latest versions) process(ID is 1) in booting process. 
daemon: system processes starts their own and keep running till the end. (Never die) 
Zombie: process killed but stil shown in system. (they don't use resources just available in process entry table)

jobs			: command list all jobs running in bg or foregrnd. 
echo $$	or ps -C bash	or pidof shell	: shows current shell PID
echo $PPID		: shows current shell parents PID 
 
fork	: splitting the process into child and executing its copy. exmaple: I am on bash --> ksh starts child process --> exec bash --> use same PID and PPID
ps a(all)u(resource usage)x(exclude daemon)	or ps -ef		: shows process detailed info	
<PID><TTY><STAT><TIME<CMND>
STAT	: shows the current status of process
D	: uninterruptible sleep (usually IO)
R	: running
S	: interruptible sleep(waiting for an event to complete)
T	: stopped (either by job signal or bcz its being traced)
X	: dead (should never be seen)
Z	: defunct(zombie) process. terminated but not reaped by parents. 
I	: idle state
Along with above stat keywards, below symbols also shown,
<	: high prioriy
N	: low priority
L	: has pages locked in memory
s	: session leader
l	: multithreaded process	
+	: foreground process
CMND	: which command has started this process.

pstree -p : shows process tree with its PID
pstree -p -u <username>	: shows processes ran by perticular user.

Signal:  signals are software interrupts delivered to a running process. They are a way for processes to communicate with each other and for the system to notify processes about various events. 
kill PID	: kills [-15 default] process. kill command has approx. 64 types of signal.(kill -l shows signal list)
SIGHUP -1	: to reread the configuration (kill -1 1 asking systemd process to reread its configuration)
SIGKILL -9	: asking kernal to kill it. In std kill command signal sent to process to kill but when we use -9 signal sent to kernal to kill process. Sure kill. 
SIGSTOP -19	: to stop process
SIGCONT	-18	: to resume the process (kill -18 PID)

pkill name	: we can stop multiple processes started with perticular name (ex: pkill sleep)
jobs
fg <jobs no.>	: to bring the bgrnd process in frground.


Day28	: Top command
system monitoring tool and provides real-time information about systems performance. 
<uptime> <no. of users logged in> <load average 1min:5min:15min>
<total tasks:	running,	stopped,	zombie>			Note: processes are divided into multiple tasks.
<%CPU	:	time spent by cpu in user space,	system space(kernal),idle state,handling hrdwr intrpts,soft intrpts,in serving other vms -steal time >
Mib MEM	:	total,	free,	used,	buffer(writing)/cache(reading)
Mib Swap:	total,	free,	used,	available	Note: swap is virtual memory used by CPU beyond RAM memory. idealy same size of RAM,doubled of RAM
<PID>	<USER>	<PR>	<NI>	<VIRT>	<RES>	<SHR>	<%CPU>	<%MEM>	<TIME>	<CMND>
PR	: priority value of process. higher the value-less priority and vice versa. always +20 of NI
NI	: nice value. used to modify process priority value. 
VIRT	: total memory consumed by process
RES	: memory consumed by process in RAM	(%MEM is % of RES)
SHR	: amount of memory shared with other processes. 

Keys:
press K	and PID to kill process
M	: to sort the process acording to mem use
P	: CPU wise
N	: process ID wise
T	: time wise
u	: user wise

process priority and Nice value:
process priority(total 139)	: used by kernal to schedule a task.
Nice value: these are user space values used to control process priority values. 
-20(Highest priority) to +19(lowest)		Note: only root user can assign minus nice values

priority wise max cpu time is allocated to processes. process priority = 20 - nice value
0-99 are used by kernal and 100-139 used by user

nice -n	num	cmand					: for new process
renice -n(new priority) <nice value range> <PID>	: for existing process


Day29: Disk Management
lsblk 	: list the block devices in system		--> lsblk -f: shows list of block devices with fs type
fdisk -l : shows detailed infor about devices + also used to create partitions (max 4)
dmesg	: gives block devices booting logs captured at the time of booting

1. How to erase perticular disk permanantly:
badblocks -ws /dev/sda or sdb <device path>	: -w(write mode test)	-s:shows % completion this command delete each secter 4 times and push random data.
or 
dd if=/dev/zero of=/dev/sda  	: makes each sector or block zero 


2. Adding block devices(SCSI(sda/sdb),SATA etc) on the go except nvme(cannot added on the go) device type:
ls /sys/class/scsi_host/ | while read host; | do echo  "- - -" > /sys/class/scsi_host/$host/scan ; done
or 
for x in `/sys/class/scsi_host/` ; do echo "------$x scanned----" echo "---" > /sys/class/scsi_host/$x/scan/ ; done

fdisk -l <block device name>		
Note: fdisk --> partition MBR disk= max 4 primary partitions or 3 parimary,1 extended partn -> its primary partitn where further logical partitions(max 5) can be created.
parimary partitions: partitions where we can keep os + other files
gdisk --> pertition GPT disk =max 128 partitions

parted command can be used to partition MBR or GPT disk.
cat /proc/partitions --> contains partition information.

fdisk /dev/sda --> if you press m then helps you to create parition based on options
n --> p(primary or e-extended)--> 1(1-4 partitions) --> 2048 (default sector)-->+1G (last sector i.e size) -->enter
if you want to chnage partition type then press t ---> to change partition type L --> select (8e for LVM type) --> w (save changes) --> enter

partprobe /dev/sda or only partprobe --> let kernal read the partition changes
lsblk 	--> to see partition created	(MAJ--> disk type number SCSI(8) nvme(259) MIN--> its partition no. 1/2/3 etc)


3. creating directory to assign to partition:
mkdir /data

4. mount /dev/sda1 /data --> will throw error bcz system dont know what fs type /data is using so before this we need to (format partition)create fs type being used by /data to store data
mkfs.xfs /dev/sda1

5. lsblk -f 	or blkid 	--> shows file types under each partition
6. mount -a (only after putting entry in fstab) or mount /dev/dsa1 /data	--> mount fs on /data				note: umount /data
Here we can use mouting options like mount -t <ext4/xfs/nfs etc> -o <default,noacl,ro for read only,noexec for no execut perm etc> 

7. df -h	--> shows mounted disk

Note: partition type 1. standard: cannot extend partition	2. lvm: can be extended
dmidecode -t1	--> shows whether its vmware or physical machine



Day30: Filesystem and mounting in Linux
FS: its way of organising the data in partition. file properties shows which file system we use.  
cat /proc/filesystem 	: shows fs supported by your system
cat /etc/filesystem	: shows auto detected fs types	(ext4 and xfs most widely used)

Journaling: dedicated area in fs where fs chnages are tracked and when system crashes then possibility of fs corruption is less due to journaling + it helps to keeps the data consistent.
ext2: 2nd extended fs. one's most widly used fs but its biggest disadvantage is that takes much time in fs checking + no journaling  + max individual file size be 16GB to 2TB + max ext2 fs size can be from 2TB to 32TB
ext3: similar to ext2 + journaling(3 types: 1.journal(metadata + content stored)	2. ordered: only metadata+ default. metadata jouranl happen only after writing content to disk	3. writeback: only metadata saved + metadata jouranl happen either before or after writing content to disk 
ext4: very big fs + can reduce fs+ journaling + max individual file size be 16GB to 16TB + max ext4 fs size can be 1EB(1024 petabytes where 1PB=1024 TB) + can mount ext3 as ext4 fs + 
xfs: cannot reduce fs + poor performance while deleting large no. of file + default fs of RHEL7 onwards
iso 9960: used to mount CDROM(CD/DVD) iso images 
gfs: always used in cluster fs

tune2fs: used to convert one fs to other (ext2 to ext3 etc)
/etc/fstab	: if need to permanently(servive reboot) mount block device then we make mount point entry in fstab file. 
<UUID=><mount directory><fs type><mode i.e read,write,acl enable etc default usually><0 for dumping i.e backup> <order in which fs check happens,put 0 for dont check fs 1(root fs),2>: here dont provide value >2 otherwise disk chk wil go in emergency.

cat /etc/mtab or mount or cat /proc/mounts	: shows current mount points in system

Note: whatever info you see under /proc these details are updated by kernal.


-----------Converting ext2 to ext3 and ext4---------------
tune2fs -l <parition> | grep feature	--> to see whether journaling supports or not

Note: below partition is mounted as ext2 fs. before converting ext we should unmount fs and then try it.
tune2fs -j <parition>					--> to convert ext2 to ext3 with journaling feature. we cannot jump from ext2 to ext4
tune2fs -O extends,uninit_bg,dir_index <partition>	--> to convert ext3 to ext4
tune2fs -O ^has_journal <partition>			--> to disable journaling
tune2fs -O has_journal <partition>			--> enable again 
 


Day32: Logical volume feature (LVM)
why lvm introduced: 
disk1(8GB)			    Partitions	
/dev/sda----------->	   1(ext2)		    	2(ext3)
		   	/dev/sda1 --> /boot	      /dev/sda2--> /
			   (4GB)			  (4GB)

disk2(50GB)			Total 10GB available
/dev/sdb-----------> 	   1(ext4)			2(xfs)
		    /dev/sdb1 -->/home		/dev/sdb2-->/srv/data
			  (20GB)			(20GB)

Here in standard physical partitions, To extend the partiton task was very hectic and complicated. (backup fs--> unmount--> format existing fs--> recreate partition --> mount --> upload backup + time to do all this etc)
thats why lvm concept was introduced to extend the partition on the go + new feature(snapshot,migrate one LVM under another LVM and merge VG, can attach disk directly to other machine, restore the deleted lvm etc)


disk1	---------> /dev/sda1 ---> pv1 ------------->    Volume group1	 ---------> LVM1(ext4)
/dev/sda	   /dev/sda2 ---> pv2-------------->		VG1			/home
								^        ----------> LVM2(xfs)
								^			/src/data
disk2  ----------> /dev/sdb1 ---> pv3---------------------------^
/dev/sdb	   /dev/sdb2 ---> pv4-------------->     Volume Group2	   -----------> LVM3 (ext4)
								VG2			/apps
								^
disk3 ----------> /dev/sdc1 ---> pv5 ---------------------------^
/dev/sdc		    (physical voumes created with physical partitions)

Note: We need physical volume to create LVM

1. add disk manually 
for x in `/sys/class/scsi_host/` ; do echo "------$x scanned----" echo "---" > /sys/class/scsi_host/$x/scan/ ; done

2. fdisk -l 
3. fdisk /dev/sda1
n --> p -->1-->  2048 -->press enter for default(full) size--> enter --> t --> L--> select 8e for LVM --> enter

4. partprobe
5. lsblk
above has created physical partition
4. pvs	: shows physical volumes available
5. pvdisplay	: for detailed physical volume info
6. pvcreate <partition>
7. vgs		: shows volume group available
8. vgdisplay	: for detailed vg info
9. vgcreate <vg name> <pv1> <pv2> etc
10. lvs or lvdisplay	: to see lvm  info
11. lvcreate -L <logical> 3G -n<new> <lvm name> <VG name>

Mount the directory on LVM
12. mkfs.xfs or mkfs.ext4 <lvm partition name> 	[example: mkfs.ext4 /dev/mapper/LinuxVG-LinuxLV   or /dev/LinuxVG/LinuxLV]
13. mount /dev/LinuxVG/LinuxLV /data
14. df -h 
15. put entry into fstab to make it permanent




--------Resizing LVM-------------
lvextend -L <total size after extend or only extended size with +sign> <partition>
lvextend -L +2000M /dev/LinuxVG/LinuxLV <-r optional>

df -h 	: will not show extended size mount we have to perform below command to make it
resize2fs <partition>				Note: we dont want to use resize2fs then use -r in above command
resize2fs /dev/LinuxVG/LinuxLV

Note: If there is no sufficient space available in VG then we can create PV and add it in VG to extend the LV.


reduce lvm: (please take backup before going ahead with this)
1. unmount partition (umount)
2. fsck -f /dev/LinuxVG/LinuxLV 	: to check file system for any errors
3. resize2fs /dev/LinuxVG/LinuxLV <total size after reduction>
4. lvreduce -L <total size after reduction> /dev/LinuxVG/LinuxLV 

-----LVM snapshot-------------
snapshot is the point in time copy of data. used it for temporary backup.
Note: in the newer systems we can directly create volume group without executing pvcreate command. vgcreate will create pv first and then add that to volume group.
lvcreate -L <size> -s(snapshot) -n(for name) partition

lvconvert --merge <snapshot lcoation[/dev/system/snap] --> restore the snapshot

lvchange -an partition 	--> to deactivate lvm
lvchange -ay partition	--> to activate it again 
mount partition <dir>

Note: modified changes in fs gets captured in snapshot so make sure you provide max value to snapshot while creating it. otherwise whenever snashot utilize % become 100 then snapshot gets corrupt.
grep Snapshot /var/log/messages --> shows snapshot corrupted 

lvextend +<size> <snapshot loca>	--> to extend the snalshot size. to avoid snapshot corrupt issue.
lvremove <snapshot location> 		--> to remove corrupted snapshot


----------Restore mistakenly removed LVM-----------
Supposed you mistakenly removed diff lvm than expected then following way you can recover that:
lvremove parititon	[lvremove /dev/LinuxVG/LinuxLV ]
ls /etc/lvm/archive	--> all changes happen for lvm are available at this position

vgcfgrestore <vg name> --test -f archive_file	: to test the restoration
vgcfgrestore LinuxVG	--test -f /etc/lvm/archive/Linux_VG_00002*.vg 

lvs				: shows restored lvm but its state will be inactive. lvscan used to see lvm status
lvchange -ay partition		: to activate lvm
mount partiton dir
df -h

Note: Before lvm restoration, dir should be unmounted. 
 


Day36: Merging and splitting Volume Groups
We have created 2 lvms in 2 separate Volume Groups.
extents:  the basic unit of allocation is called an "extent." An extent is a contiguous range of physical extents on disk and is used as the building block for creating logical volumes.
while creating lvm we have used -l because we are not giving total lvm size. we use <extents> to create lvm in linux. defaults extents is 4MB in linux.
lvcreate -l 1000 -n <lvmname> <vgname>		--> here 1000 X 4MB extents we are creating which is 4GB LVM. this means in lvm we can divide lvm in 4,4MB area.


lvcreate -l 1000 -n lv1 vg1			--> -l 1000 specifies the size in terms of extents and -L 1G specifies the size in gigabytes.
lvcreate -l 1000 -n lv2 vg2

lvs
mkfs.ext4 /dev/vg1/lv1				--> formating fs to mount to dir
mkfs.xfs /dev/vg2/lv2


mkdir /test1
mkdir /test2

mount /dev/vg1/lv1 /test1
mount /dev/vg2/lv2 /test2

---here if we want to merge 2 VG then we need to unmount lvm of both VG.
umount /test1
umount /test2

after unmounting, you stil see lvm are still in active state. so we need to deactive the lvm which we need to merge with another lvm.
lvscan		--> to see lvm status

vgchange  -an vg2	--> to deactivate lvm2 of vg2
lvscan 	
vgmerge vg1 vg2		--> vgmerge vg1<with whom you want to merge> vg2<which vg you want to merge>
vgs			--> shows only one vg now
vgchange -ay vg2	--> to actvate lvm2 of vg2	or you can also activate using lvm command lvchange -ay /dev/vg2/lv2

now we can mount the directories again which we unmounted before
mount /dev/vg1/lv1	/test1
mount /dev/vg1/lv2	/test2



--------Splitting the vg into multiple vg
In above example, we have lvm1 and lvm2 part of vg1 only. we can split vg1 into vg1 and vg2.
1. unmount /test1 and /test2
umount /test1
umount /test2

2. deactivate the vg1 which we have to split
vgchange -an vg1

3. vgsplit vg1<which we need to split> vg2<new vg> /dev/sda2 <physical volume>
4. vgs
5. vgscan	--> to see status
6. activate the vgs
vgchange -ay vg1 vg2

7. mount /test1 and /test2
mount /dev/vg1/lvm1 /test1
mount /dev/vg2/lvm2 /test2



Day37: Migrating physical volume and LVM from one disk to other on the go
We are moving lvm1 from disk1 to disk2. Consider that you have lvm1 configured under vg1 (disk1). if we have to move lvm1(Vg1-disk1) to disk2 then we should create lvm partition + physical volume under it.
lsblk
sda					disk
 --> sda1				part
       ----> dataVG-dataLV		/data
sdb					disk
 --> sdb1				part

Moving dataVG-dataLV to sdb1 partition
1. pvcreate /dev/sdb1			--> creating physical volume
2. vgextend dataVG /dev/sdb1		---> to extend the Volume Group with physical volume where we want to move disk lvm
3. dmsetup deps /dev/dataVG/dataLV	--> to check lvm1 dependancy, if has any then it can trigger an issue while moving to disk2
4. pvmove -n <lvmname> <source pv> <target pv>
pvmove -n dataLV /dev/sda1 /dev/sdb1			--> moving data from physical volume sda1 to sdb1
5. vgreduce <vgname> <pv name which we need to reduce>
vgreduce dataVG /dev/sda1				--> bcz we have moved data from /dev/sda1 to sdb1


--------Migrating lvm data disk from one machine to another------
1. deativate the volume group(vg)
vgchange -an <vg name>	--> vgchange -an Volgrp
2. export the vg			--> after export if you run lvscan or lvs it should not show you VolGrp in result. 
vgexport Volgrp
3. powerof machine
init 0
4. add above exported disk to new machine
5. scan the vg or pv using pvscan or execute below command
for x in '/sys/class/scsi_host/' ; do echo "-------$x Scanned-----" echo "--->/sys/class/scsi_host/$x/scan/" ; done
6. you can see vg in exported mode so import it first
vgimport VolGrp
7. after importing you will see x option from vgs has been removed. but vg still in deactivate state.
vgchange -ay VolGrp
8. mount exported lvm and access 
mount /dev/VolGrp/LogLVM /data


Day39: Setup thin provisioning volumes in LVM 
Thin provisioning is the concept through which we are using storage space effectively.You can manage the storage pool(LVM) of free spaces called a Thin pool which can
be allocated to number of devices when needed by an application instead of extending the storage pool(VG).

VG	--> volume group is created by deviding the multiple 4MB extents in storage spaces. VG will create 4MB physical extents by default if we not provide physical extend space using -s option. 
vgcreate -s 32M thin_vg /dev/sdb1
lvcreate -L 15G --thinpool tp_thinpool thin_vg		--> we are cerating LVM only just adding --thinpool to make it.

Now we will create 3 LVM of 5G to use the pool
lvcreate -V 5G --thin -n thinpool_clien1 thin_vg/tp_thinpool			--> -V specify size of thinpool
lvcreate -V 5G --thin -n thinpool_client2 thin_vg/tp_thinpool
lvcreate -V 5G --thin -n thinpool_client3 thin_vg/tp_thinpool

mount the dir on above created thin provisioned lvms
mkdir -p /mnt/client1 /mnt/client2 /mnt/client3

mkfs.ext4 /dev/thin_vg/thinpool_client1 && mkfs.ext4 /dev/thin_vg/thinpool_client2 && mkfs.ext4 /dev/thin_vg/thinpool_client3
mount /dev/thin_vg/thinpool_client1 /mnt/client1 && mount /dev/thin_vg/thinpool_client2 /mnt/client2 && mount /dev/thin_vg/thinpool_client3 /mnt/client3

dd if=/dev/zero of=/mnt/client1/text.txt bs=1024M count=4 	--> create 4GB dump data

lvcreate -V 5G --thin -n thinpool_client4 thin_vg/tp_thinpool 	--> over-provisioning. we are creating 4th LVM with 5GB space. If its not --thin pool then it will throw error as vg does not have enough memory. In thinpool system will allocates unused memory from other LVM's to lvm4. But here we have to keep monitoring the vg_pool memory threshold.
if vg_pool 100% occupied then lvm will get corrupt.

lvextend -V +15G /dev/thin_vg/tp_thinpool	--> extend thin pool with 15GB more. but to make sure VG has enough memory before adding it. otherwise create pv on disk and add it to vg.

	

Day40: Manage multiple LVM disk using striping I/O
LVM striping	--> Its one of LVM feature which will write on multiple disks instead of constant write on single disk. 
benefits: (disk performance increases + increase disk life + reduce disk fillup issues etc) 

disk1		partition	physical volume
/dev/sda--> /dev/sda1	---> 	pv1	-----------⌄	
	70 I/O					   ⌄
						___⌄__________
disk2					       |             |
/dev/sdb---> /dev/sdb1	---> 	pv2  ------->  |Volume Group |	---> LVM (striping I/O) ---> mount dir
         70 I/O				       |_____________|	
						   ^
disk3						   ^
/dev/sdc --> /dev/sdc1 ----> 	pv3   -------------^
         70 I/O


we consider that we have volume group created with 4 PV(1GB each) in it. 
lvcreate -L 900M -n lv_striping -i4 striping_vg			--> -i4 used to create 4 striping lvm

lvdisplay striping_vg/lv_striping -m			--> see striping lvms using -m option. default striping size is 64 KB

lvcreate -L 1G -i3 -I 256 -n lv_striping2 striping_vg /dev/sda1 /dev/sdb1 /dev/sdc1  		--> here I used to set striping size + which 3PV's should be used for striping.
 


Day41: Logical Volume Manager(LVM) commands
cd /etc/lvm -->found below directories
archive --> vg metadata changes are stored here. using this we can restore deleted lvm
backup  --> stores backup of lvm
cache  --> chached lvm content
profile --> like user profile files, lvm profile files are stored here.
lvm.conf --> used to configure lvm related configurations. 

/var/lock/lvm  --> files which are used to prevent lvm fs corruption. 

pvck /dev/sba1  --> used to check physical volume consistancy
pvchange -an /dev/sda1   --> used to change lvm attributes. like activate, disable pv etc

vgrename/lvrename oldname  newname	--> to rename vg/lv
lvs -a -o +devices		--> used to lvm details along with devices 


To remove partition --> 
1. unmount directory
2. lvremove /dev/old_vg1/old_lv1
3. vgremove old_vg1
4. pvremove /dev/sda1
5. fdisk /dev/sda --> d(delete)

 
pv attributes:
pvs --> shows 3 dash (---). first is(a-> allocated, u-> unallocated,--- means not used etc)
pvchange -x n /dev/sda1    --> lvs command will show u-- in attributes. here n means no. 


vg attributes:
r,w --> read, write
z   --> resizable
x   --> exported
p   --> partial 

vgcreate options:
-l --> max logical volume
-p --> max physical volume
-s --> physical extent size(default 4MB)
-A --> autoBackup

lvcreate options:
m --> mirrored
M --> mirrored without intial sync
o --> origin
p --> pvmove
s --> snapshot
S --> invalid snapshot
i --> mirror image
I --> mirror image without sync
- --> simple volume



Day42: Create and Manage Swap(file type) Space
Swap space is a virtual space on hard disk which is used as substitute for physical memory and can be used when real RAM fills up and more space is needed. 
1. fdisk /dev/sda
2. n --> p --> 1--> 2048 --> 3G -->w --> t --> L --> select 82 for swap file type --> enter
3. partprobe
4. lsblk -l
5. mkswap /dev/sda1
6. swapon /dev/sda1
7. lsblk --> will show /dev/sda1 as swap fs type
8. cat /proc/swaps  or free -m or top	--> to check swap space 
9. add entry in /etc/fstab to servive the reboot. 

In above example, we have created swap on physical partition but if we need to extend swap then there will be issue. 
so better to make swap on lvm

1. fdisk /dev/sdb
2. n --> p --> 2048 --> 3G -->w --> t --> L --> select 82 for swap file type --> enter
3. partprobe
4. lsblk -l
5. pvcreate /dev/sdb1
6. vgcreate vg /dev/sdb1
7. lvcreate -L 3G -n swap2 vg 					--> lvextend -L +4G /dev/vg/swap2 (to extend swap lvm by 4G)
8. mkswap /dev/sdb1
9. swapon /dev/sdb1
10. lsblk --> will show /dev/sdb1 as swap fs type
11. cat /proc/swaps  or free -m or top	--> to check swap space 
12. add entry in /etc/fstab to servive the reboot. 

Note: never reduce swap space if its on same /root fs lvm

  
----Swap usage and its working, swappiness-------------
Swapping is the process to copy chunks of memory(pages) from RAM to predefined space in hard disk. this is also called paging.   
Total virtual memory in system = RAM size + Swap space

why swapping:
1. when we need more physical memory than existing RAM --> swaps out less utilized pages and gives memry to current application.
2. Most of the applications use RAM for initialization and after that they dont use RAM. so system can move these application in swap space and use memory for new applications. 

disadvantage:
1. its slows the performance of system. RAM--> speed in ns 		Hard Disk --> speed in ms

vmstat  	--> to check pages in and out details. make sure sysstat package is installed 
swappiness	--> is the parameter(range 0-100) intro after RHEL6 to tweak swapping configuration(how freq swapping shd happen etc). RHEL6,7 -> default value is 60 and RHEL8 --> default value is 30
cat /proc/vm/sys/swappiness 	--> to see default value. to inc swap utilization num shd be higher. for less swapping --> less number

echo 50 > /proc/vm/sys/swappiness	--> use to change default swap value temporarily.   
vi /etc/sysctl.conf			--> vm.swappiness=10	--> set permanently 


----use file as swap space-----
we can create file in /root fs and use it as swap space
1. dd if=/dev/zero of=/swapfile bs=1024 count=1048576
or
fallocate -l 1G /swapfile					--> create empty file in root (ideally we should create at location with high memory)

2. chmod 600 /swapfile			--> only root can read file
3. mkswap /swapfile			--> format /swapfile as swap fs type
4. swapon /swapfile -v			--> to enable swap
5. free -m or cat /proc/swaps
6. vi /etc/fstab:
/swapfile	swap	swap	defaults	0 0
    

-----remove swapfile-------
1. erase swap entry from /etc/fstab
2. swapoff /swapfile -v
3. free -h or swapon --show
4. rm -f /swapfile


-------How to use file as virtual hard disk----------
stress	--> used to impose load on system. totally 4 types of load (CPU,RAM,I/O,DISK)
we need to install stress package first. to install stress package we use extra package repo(epel-release) and then install stress.
dnf install -y epel-release 
dnf -y install stress

stress --cpu 4 --timeout 20
stress -m 1 --vm-bytes 4G			--> based on swappiness parameter we can see after a stress load how swap space is being used when RAM is about to get full. 
 

--use file as virtual hard disk----
1. dd if=/dev/zero of=VHD.img bs=1M count=1024		--> 1GB file we are creating
2. mkfs -t ext4 VHD.img
3. blkid
4. mkdir /testdata
5. mount -t auto -o loop VHD.img /testdata		--> if you dont know options you can only write mount -a
6. put entry in fstab
VHD.img /testdata	ext4	default 0 0		--> never mention UUID in fstab for virtual hard disk. always put full location


umount /testdata + delete VHD.img 	---> to unmount virtual hard disk


Day45: RAID and RAID levels(RAID 0)
RAID --> redundant array of independent drives. Its technology used to increase performance and reliability of data storage. RAID system consist of 2 or more drives working together in parallel.
RAID 0 ---> striping (increased performance + no overhead + no fault tolerent) --> ideal for no critical data storage + higher performance requirement
RAID 1 --> mirroring 
RAID 5 --> striping with parity
RAID 6 --> striping with double parity
RAID 10  --> combining striping and mirroring 

striping" refers to a method of data distribution and storage across multiple disks in the RAID array.

Create RAID partitions:(mdadm)
1. fdisk /dev/sda
n--> p--> 1--> 2048 --> 2G --> enter --> w--> t--> L--> fd(RAID file type) --> enter
n--> p--> 2--> 2048 --> 2G --> enter --> w--> t--> L--> fd(RAID file type) --> enter
2. partprobe /dev/sda1 /dev/sdb1
3. cat /proc/mdstat	--> to see if any RAID disk configured
4. mdadm -C(create) /dev/md0 -l(level) 0 -n(no.) 2 /dev/sda1 /dev/sdb1			--> We can also use raw disk to create RAID
5. lsblk 		--> show you RAID partition
6. mdadm -D /dev/md0	--> print detailed RAID info 
7. mkfs.ext4 /dev/md0	--> format RAID partition
8. mkdir /testRAID
9. mount /dev/md0 /testRAID
10. put entry in fstab
/dev/md0	/testRAID	ext4	defaults 0 0


1. unmount dir
2. mdadm -S /dev/md0	--> to stop RAID



-------Create and Manage RAID1-----------
1. fdisk /dev/sda 	+ fdisk /dev/sdb  + fdisk /dev/sdc
n--> p--> 1 --> 2048 --> 2G --> enter --> w--> t--> L--> fd(RAID file type) --> enter
n--> p--> 2 --> 2048 --> 2G --> enter --> w--> t--> L--> fd(RAID file type) --> enter
n--> p--> 3 --> 2048 --> 2G --> enter --> w--> t--> L--> fd(RAID file type) --> enter

2. partprobe /dev/sda1  /dev/sdb1 /dev/sdc1
3. cat /proc/mdstat	--> to see if any RAID disk configured
4. mdadm -C(create) /dev/md0 -l(level) 1 -n(no.) 3 /dev/sda1 /dev/sdb1	/dev/sdc1		--> We can also use raw disk to create RAID
5. lsblk 		--> show you RAID partition
6. mdadm -D /dev/md0	--> print detailed RAID info 
7. mkfs.ext4 /dev/md0	--> format RAID partition
8. mkdir /testRAID
9. mount /dev/md0 /testRAID
10. put entry in fstab
/dev/md0	/testRAID	ext4	defaults 0 0

1. mdadm /dev/md0 -f /dev/sda1 		--> to make the disk faulty
2. unmount /testRAID
3. mdadm /dev/dm0 -r /dev/sda1		--> to remove disk from RAID1 
4. mdadm /dev/md0 -a /dev/sdc1		--> to add new disk to RAID
5. mount /dev/md0 /testRAID


--------Create and Manage RAID5-------
min 3 disks + max 16 disks used in RAID5 + here dedicated disk used for parity checksum(used for error detection and error correction)
fast read transactions + litle slow write + 1 disk failure can withstand(if using 3 disks) --> advantages
complex technology + data can loss if >1 disks failed at same time --> disadvantage
RAID2 + RAID3 + RAID7 --> rarely used

RAID file capacity is always = number of disks used for stripping but not for parity
mdadm -c /dev/md0 -l 5 -n 3 /dev/sda /dev/sdb /dev/sdc 	--> using raw disks to create RAID5 partition
mkfs.ext4 /dev/md0
mkdir /raid5
mount /dev/md0 /raid5
mdadm -D /dev/md0  or cat /proc/mdstat

mdadm /dev/md0 -f /dev/sda	--> to make disk falty
mdadm /dev/md0 -r /dev/sda	--> remove faulty disk
mdadm /dev/md0 -a /dev/sdc	---> add new disk to RAID5



-------Create and Manage RAID6----------
min4 disks + stripping with double parity(dedicated disk is used for parity and that parity backup is saved on other disk) 
if 2 drive fail even withstand(is using 4 disks) + efficient storage + safe data is concern then use RAID6 --> preffered over RAID5
20% slower than RAID5

same steps as above

-----Create and Manage RAID1+0-----------
if one drive fails then rebuilt time is fast as you just have to copy data from mirror disk to new disk --> advantage
half of drive goes into mirroring so comparing with RAID5/RAID6, this is an expensive way of redundancy --> disadvantage

stripping --> happen at upper level and Mirroring -> happens at lower levels
Say we have 4 disks used in RAID10 then there will be 2 sets(SetA(1-1 disk) and SetB(1-1 disk).



Day50: Disk quota management for users and groups in ext4 fs
Disk quota management concept is used to set limits on how much disk space or how many inodes one can use. helps to efficiently use file storage. 
Quota policy:
soft limit : gets warning before full limit used 
hard limit : can't use after this limit
grace period: additional time given to user or grp to use space or inodes

you need to first check quota packages are installed or not:
rpm -qa quota or yum list installed quota
 
3 types of quota can be specified:
1. how much of block space can use
2. how many files one can create by specifying inode value
3. both

1. create partition and mount dir
vi /etc/fstab
/dev/sda1 	/quotadir	ext4	default	0 0
2. mount -a			--> mount quotadir on /dev/sda1
3. mount | grep quota		--> to see whether quota enable on disk or not,we have not enabled so wont show anything.
4. vi /etc/fstab
/dev/sda1 	/quotadir	ext4	default,usrquota,grpquota,prjquota	0 0
5. mount -o remount /quotadir			--> remount 
6. mount  grep quota		--> will show you quota enabled on dir
7. quotacheck -cug /quotadir	--> to create quota files for user and groups, which can be used to set quota limits for user and grps
8. quotaon /quotadir		--> enable quota
edquota	<username>		--> to specify user quota
edquota	-g <grpname>		--> to specify quota for grp
9.quota <username>		--> check quota for users

edquota -T <user> or -g <groupname>	--> default is 7 days. to set grace period for user or grp --> make sure you make quotaon /quotadir after this. else if made  on earlier then make off, specify grace period and make it on.


-----xfs_quota expert utility to manage quota on xfs fs---------------
xfs_quota -x(expert i.e batch mode without UI) -c(commandline) report /quotadir			--> shows quota specified on /quotadir
xfs_quota -x -c 'limit bsoft=  bhard=  <username>' /dir			--> set block quota limits for user on /dir
xfs_quota -x -c 'limit isoft= ihard= <usr>' /dir			--> set inode quota limits for user on dir
xfs_quota -x -c 'report bih' /dir					--> to see block and inode quota in human readable format

xfs_quota -x -c 'limit bsoft=  bhard=  <username>' /dir  -c 'limit isoft= ihard= <usr>' /dir		--> set block or inode limit quota in single cmnd


------prjquota used to implement quota limit on dir------------
echo projectname:ID >/etc/prjid
xfs_quota -x -c 'project -s projectname' /dir				--> enable project quota
xfs_quota -x -c 'limit -p bhard= bsoft= prjname' /dir			--> set quota limit on project
xfs_quota -x -c "state -p" /dir						--> to see project quota
xfs_quota -x -c "timer -p<project> -b<block> 10days" /dir		--> reduce grace period using timer on project dir
xfs_quota -x -c "off/disable/enable -pugv" /dir				--> disable/enable quota for project,user and group v-verbose. use off for permanent disable.


-----Limit the process on user level-----
ulimit -a		--> shows current limits for core file size, data size, file size, open files, pipe size, stack size, CPU time, max user processes, virtual memory, and more.

ulimit -u <limit>
/etc/security/limit.conf	--> file used to set up user limits permanently. file contains 
<domain-user/uid,group/gid> <type-soft/hard> <item-resources like nproc for process> <value>



Day52: Check and repair FS
fsck,e2fsck(ext fs), xfs_repair(xfs fs)	--> used to check and repair fs.fsck is wrapper on e2fsck. e2fsck was used to chk ext2 fs but later extended to ext3,4.
rpm -qa | grep utils-linux*		--> package resposible for fsck commands. rpm -qf "which fsck" 	--> tells you pcg for fsck commands

Note: fs should not be mounted to check fs. below options are used with fsck
-f	: to forcefully check even disk is clean
-v	: verbose
-a	: to fix if errors any
-n	: dont repair error
-A	: to check all fs in system in order specified in fstab
-R	: exclude root fs


Day53: Virtual data optimizer(VDO) volume
its block virtualization technique + provided data de-duplication and compression facility at block level + can also provide thin provisioned volumes using VDO.
yum list installed kmod-kvdo*				--> need this package for VDO
yum install kmod-kvdo vdo -y				--> install if not available

systemctl start vdo					--> start vdo service to create thin prvisoned volumes
vdo create --name=vdo1  --device=/dev/sda  --vdoLogicalSize=300G		--> create VDO
vdostats -hu						--> shows vdo details
pvcreate /dev/mapper/vdo1				--> created pv
vgcreate vdovg1 /dev/mapper/vdo1			--> created vg of 300GB where pv is of 10GB only

lvcreate -L 50G -n vdolv1 vdovg1			--> create lv 
mkfs.ext4 -k /dev/vdovg1/vdolv1				--> format lv
mount -o discard /dev/vdovg1/vdolv1/ /testvdo		--> used discard option to discard unused blocks from partition.
		
Note: VDO provides space-saving features by deduplicating and compressing data, allowing you to have a larger logical size than the physical capacity of the underlying disk. The actual physical disk usage will depend on the nature of the data being stored. If there is a lot of redundant or compressible data, VDO can achieve significant space savings.


Day54: stratis filesystem (RHEL8 exclusive feature)
Stratis and LVM thin pools serve similar purposes.
default xfs fs use it + can take snapshot + thin provisioning + pool based storage mgmt + fs and storage monitoring we can do in stratis fs
2 statis packages are needed: stratisd --> deomon services used to start stratis services 	stratis-cli --> manage stratis using cli

yum install -y stratisd stratis-cli 		--> if not installed
systemctl start stratisd			--> start and enable stratisd
systemctl enable --now stratisd

lsblk -f					--> to see block devices with fs types
wipefs -a(all) /dev/sdb				--> we can wipe all fs if there exist any on /dev/sdb
stratis pool create stratis_pool1 /dev/sdb (multiple disks)	--> create stratis pool from existing drives 
stratis pool list				--> to list available pools 

stratis fs create stratis_pool1 filesystem-1	--> create fs on stratis pool by formating it
stratis fs list <poolname>			--> show stratis fs

mount /dev/stratis/stratis_pool1/filesystem-1	/data		--> mount stratis fs
vi /etc/fstab:
/dev/stratis/stratis_pool1/filesystem-1		/data
mount -a							--> mount startis fs

stratis fs destory <poolname> <fs-name>				--> to destroy fs from pool
stratis pool add-data <pool name> <disk>			--> to add disk to pool [stratis pool add-data stratis-pool1 /dev/sdb]


stratis fs snapshot stratis_pool1 filesystem-1 mysnapshot1	--> create snapshot from filsystem-1 as mysnapshot1
mount /dev/stratis/stratis-pool1/mysnapshot1	/data		--> to restore file system

rm /dev/stratis/stratis_pool1/mysnapshot			---> to remove snapshot

stratis pool destroy stratis_pool1				---> to destroy the startis pool

 

Day55: Set up disk encryption usng LUKS(utility) in Linux (like BitLocker in windows)
LUKS --> linux unified key setup. used to encrypt the data on disk. 
dnf list installed | grep cryptsetup			--> checking LUKS utility package installed or not. if not then install it

cryptsetup luksFormat 	/dev/sdb			--> formating /dev/sdb disk for encrypting it.
cryptsetup luksOpen	/dev/sdb <name of fs on /dev/sdb>	 --> name can be anythng like myfiles etc
cryptsetup status myfiles				--> to check fs status

mkdir /data
mkfs.ext4 /dev/mapper/myfiles

mount /dev/mapper/myfiles	/data			--> mount encrypted disk

---to permanently mount encrypted fs-----
dd if=/dev/urandom of=/temp/crypt_file bs=4096 count = 1			--> creating key file which can be used to encrypt disk. provide password
chmod 600 /tmp/crypt_file

mv /tmp/crypt_file /etc/

cryptsetup luksAddKey	/dev/sdb  /etc/crypt_file			--> define key should be used on which disk to encrypt. 
vi /etc/crypttab							--> file used to mount crypted disk permanently
<name of fs>	<disk>		<key file>
myfile		/dev/sdb	/etc/crypt_file	  luks			

vi /etc/fstab
/dev/mapper/myfiles	/data	ext4 default 0 0

mount -a


cryptsetup luksClose myfiles					--> close encrypted disk
cryptsetup luksRemoveKey	/dev/sdb			--> remove key from disk
cryptsetup luksRemove myfiles					--> remove encrypted fs 
cryptsetup luksChangeKey	/dev/sdb			--> if want to change key which is used to encrypt disk




Day56: RHEL8 Booting process
Booting --> the process of copying os files from hard disk(/boot parition) to RAM sequencially and allowing us to use system  and other os components. 

BIOS 	--> When we start machine, singal flows in motherboard. BIOS prg in chip(CMOS battery) executes POST operation. 
 ⌄
 ⌄
POST  --> power on self test. it checks whether all the attached compoents working or not. and gives control back to BIOS
 ⌄
BIOS 	--> when it gets the POST result as successful then it executes MBR by searching in boot devices (HDD,LAN,CD/DVD,USB etc).
 ⌄		 if there is any h/w issue then system fails.  
 ⌄
MBR	--> Master boot record stored in 1st sector of booting devices.(512byes) primary boot loader information stored in 440 byes of code. MBR is not a main boot loader. rather main boot loader sector info stored here. (primary boot loader points to GRUB2 i.e main boot loader)
 ⌄
 ⌄
GRUB2	--> Grand unified loader(v2).  GRUB2 loads its conf. from /boot/grub2/grub.conf file + then provides option to select kernal from boot menu to boot machin 
 ⌄	    then loads selected/default VMLinuz kernal img from /boot/vmlinuz-4.18* + then extract contents of initramfs img frm /boot/initramfs-4.18*
 ⌄		(loads kernal into RAM)
	   /boot/grub2/grub.conf --> this contains info related to grub timeout,default kernal, default run level etc but never edit file directly
Kernal
 ⌄
--> Kernal(VMLinuz) find drivers in initramfs for all h/w intialization/ after this it executes the init(systemd) process
H/W initialization
 ⌄
 ⌄
/sbin/init execution	--> Kernal executes /sbin/init from initramfs which will start 1st process having PID-1 + in RHEL8 init has been replaced with systemd
	/sbin/init has soft link to ../lib/systemd/system  + its the process which keeps running until system up and stops at last.
 ⌄
initrd.target execution	--> with the help of initramfs systemd executes all units for the initrd.target + In RHEL8 runlevel called as targets. 
 (total 7 targets) + this includes mounting the root fs on disk at /sysroot dir for temp 
 ⌄
Switches root fs	--> Kernal root fs switch from initramfs root(/sysroot) to system root fs(/)
 ⌄
 ⌄
systemd looks for default target  --> system reads the file(/lib/systemd/system/*target) linked by /etc/systemd/system/deafult.target to determine default target(run level)
		systemctl set-deafult <target> --> used to set default run level
 ⌄
1. /lib/systemd/system/runlevel0.target --> poweroff.target
2. /lib/systemd/system/runlevel1.target --> rescue.target
3. /lib/systemd/system/runlevel2.target --> multi-user.target
4. /lib/systemd/system/runlevel3.target --> multi-user.target
5. /lib/systemd/system/runlevel4.target --> multi-user.target
6. /lib/systemd/system/runlevel5.target --> graphical.target
7. /lib/systemd/system/runlevel6.target --> reboot.target
 ⌄
 ⌄
Start other services and OS components 		--> system target file defines the services that system starts



Day57:  Set or Change default run level
1. /lib/systemd/system/runlevel0.target --> poweroff.target		--> shut down + power off 
2. /lib/systemd/system/runlevel1.target --> rescue.target		--> single user mode + without GUI, network
3. /lib/systemd/system/runlevel2.target --> multi-user.target		--> without GUI,n/w
4. /lib/systemd/system/runlevel3.target --> multi-user.target		--> Without GUI but n/w
5. /lib/systemd/system/runlevel4.target --> multi-user.target		--> only for research use only
6. /lib/systemd/system/runlevel5.target --> graphical.target		--> with n/w
7. /lib/systemd/system/runlevel6.target --> reboot.target		--> reboot

runlevel --> command to see runlevel or who -r
systemctl get-default		--> get current permanent default target
cat /lib/systemd/system/default-target			--> shows current target
init <no>			--> imediatly change run level. it has no impact on default run level


----Resolve kernal panic error------------
Kernal panic error --> due to initramfs files corrupt or deleted 

ls /boot		--> shows kernal files
/etc/grub.d or /etc/default/grub 	--> used to update grub configuration 
grub2-mkconfig		--> after updating above files we run this to make changes in grub confguration  

init 6 or systemctl reboot 		--> reboot machine

-----If we get kernal panic error due to initramfs file corrupt or deletion-----
1. use grub menu to login through rescue  mode
2. cd /boot 	--> to check initramfs file deleted or corrupted
3. uname -r	--> to check os version
4. dracut -v <initramfs-(paste above result).img> <above result>	--> to create initramfs file use -f to override in case corrupt
or 
mkinitrd -v --force initramfs-(uname-r-result).img (uname-r-result) 	--> to regenerate intiramfs files
5. reboot machine


----If grub.conf deleted------
In this case we can't see grub menu to login so follow below steps to fix it.
1. boot os with img file which we have used to install os
2. select troubleshoot option
3. continue (to boot machine using /mnt/systroot)
4. get the shell option
chroot /mnt/sysimage				--> we use chrooting to chnage in os files
grub2-install <device i.e /dev/sda>		--> to install grub2 directory
grub2-mkconfig -o /boot/grub2/grub.conf		--> to generate grub.conf
touch /.autorelabel				--> relabel selinux context
exit


-----Recover VMLinuz files--------------
If VMLinuz file got corrupted or deleted from /boot then follow below steps to recover
1. login to system using grub menu rescue option
2. cd /boot
3. reinstall VMLinuz package 		--> cd /os-img/baseOS/packages/kernel-core-4*
dnf -y reinstall kernel-core-4*.rpm
4. reboot machine


----If both VMLinuz and rescue VMLinuz deleted-------
1. boot through iso image
2. select troubleshoot option
3. select 2nd option centos system
4. select continue (/mnt/sysimage)
5. cd /mnt/install/baseOS/packages/
6. reinstall VMLinuz package 		--> cd /os-img/baseOS/packages/kernel-core-4*
rpm -ivh --root=/mnt/sysimage --replacepkgs kernal-core-4*
7. touch /mnt/sysimage/.autorelabel



--change grub timeout---
vi /etc/default/grub
change timeout to 25 sec
grub2-mkconfig -o /boot/grub2/grub.conf
init 6



-----Cleanup /boot directory-------
dnf/yum update			--> to update the repo
dnf/yum remove <old kernal>
rm -f <kernal pkg + initrafs file from /boot dir>



Day62: Schedule future jobs using AT
AT 	: used for non repeatative jobs to schedule in future. one time jobs
crontab : used to schedule repeatativ jobs

rpm -q at or yum list installed at		--> to check at pckg installed or not
systemctl status atd.service			--> to check at deamon running

at <time>	--> to schedule job at perticular time. [at now +1 min]
at command or script > file or terminal		--> which will run on above scheduled time. can get terminal info by executing w command 

atq 	or at -l 		--> to list scheduled jobs
at -c <job no>			--> to see jobs details
ls /var/spool/at		--> shows scheduled filed, we can chnage these files to make changes in scheduled cmmand

tail /var/log/cron		--> to see cron jobs logs
atrm <job no>			--> to remove job from at queue

---scheudle jobs on diff times----
at noon +2 min/hours/days
at 7PM +60 days		or at 7PM +2 months
at 00 AM 08/01/2023				--> MMDDYY
at 9:00 AM -f script.sh	> /dev/pts/0		--> print script output on terminal 
at 01 AM -M					--> mail the output. use -m to mail if command failed
at -t 202307120130				-- at -t yyyymmddhhmm 


----------crontabs-------------
powerful utility to schedule non + repeatative jobs 
rpm -q crontabs				--> to check crontab pakge
systemctl status crond.service		--> make sure cron daemon is running 

* * * * * 		--> 1st (*): minute(0-59)	2nd: hour(0-23)		3rd: day of month(1-31)		4th: month(1-12)	5th: day of week(0-6) 0-Sunday
crontab -e		--> edit crontab press i
37 9 * * * echo "Shantayya is smart guy" > /dev/pts/0  
@reboot echo "Shantayya is smart guy" > /dev/pts/0		--> use @ to specify when it should execute

crontab -l 		--> shows cron jobs available based on username. 
crontab -l -u <username>	--> must have sudo access to list other usrs cron jobs
ls /etc/cron.*		--> shows cron dir in which jobs enries happen

cat /etc/cron.deny	--> we can limit users to schedule jobs by making user entries in this file.

ls /etc/anacrontab	--> this file is used to schedule jobs using anacron utility on machine which dont run 24X7
crontab -r		--> to remove cron jobs for self account. append -u <username> to remove cron jobs for other users.

------few cron entries------
*/2 * * * * 		--> every 2 min
* */2 * * * 		--> every 2 hrs
0 2 * * *  		--> daily at 2am (*--> means every)
0 5,17 * * * 		--> daily twice at 5AM and 5PM
0 1 1 1,5,8 *		--> only in Jan,May and Aug
0 17 * * 0,5 		--> every sunday and friday. 
0 2 1-7 * * [ `date + \%u` = 7]	&& cmnd --> 1st sunday of every month. 1-7 represents 1st week

* * * * *
* * * * *  sleep 30; 		--> every 30 sec

use ; to schedule multiple tasks
@yearly			--> every year
@monthly 		--> every month
@weekly, @daily, @reboot 

crontab -l > backupfile.txt 			--> take crontab backup
crontab backupfile.txt				--> restore cron jobs

 

Day66: Resource monitoring, management and troubleshooting (CPU,N/W,RAM,STORAGE)
---cpu----
top, free -g, free -s 3 --> to update result every 3 sec
watch -d<delay> -n<no.> 4<sec> cmnd		--> to repeat command every 4 sec. [watch -d -n 4 free -g]
mpstat			--> shows cpu utilization. -P ALL --> for each core utilization

---RAM----
lsof 			--> list of open files. 
lsof -p <pid>		--> to see open files by pid
lsof -i :port_no	--> to list files opened on port number
lsof -u <username>	--> list files with username 


----storage---
iostat		--> used to check input output details. for this to work system should have sysstat package. 
chroot		--> it creates an env(shell) through which we can access root fs directories. 



Day67: package management 
.deb	: extension for debian based packages. --> dpkg, aptitude, apt-get --> comands used to install packages. 
.rpm	: extension for rhel based packages. ---> rpm,yum, dnf	--> utility used to install packages. 

repo	: collection of software and its documentation at central place. 


---debian pkg managemnt-----
dpkg -l		--> shows list of packages installed on machine
dpkg -S rsync	--> to see files installed as part of rsync pkg
dpkg -i httpd	--> to install pkg. -r --> to remove it

Note: only issue with dpkg utility is user have to manually keep track of package dependacies.

apt-get	update/upgrade		--> update pkgs + upgrade makes further enhancement in pkg version plus functionality.
ls /var/cache/apt/archive	--> location where packages are downloaded.
apt-get search rsync		--> search pkg in downloads dir
apt-get install rsync		--> install rsync pkg. remove --> for un install  purge	--> to remove pkg with its conf file.

ls -rt /etc/apt/sources.list	--> contains online repostory urls
add-apt-repository	<url>	--> to add new online repo
or
apt-key add <url> 	

apt-get install alien		--> alien pkg used to convert rpm to deb pkg. Note: but necessary that converted pkg will work. 
alien --to-deb <rpm pkg name> 


---fedora pkg management-----
rpm -qa	| grep samba		--> query all pkg with name samba
-qi				--> query installed : used to see deatailed info of pkg
-i 				--> to install pkg
-e				--> to erase pkg
-Uvh				--> upgrade pkg. v-verbose h-hash(completion %)

ls /var/lib/rpm			--> keeps track of installed pkg(metadata)
rpm2cpio pkg | cpio -t		--> to list archive file containts

Note: issue with rpm utility is that it cannot solve dependancy by itself. 

dnf(RHEL8-dandified yum): advanced version of yum + faster + consumes less memory than yum + written in python 
yum --> to fix dependancy issue in rpm. 

dnf list pkg		--> to search pkg 
dnf list installed	--> installed pkges 
dnf install/remove pkg	--> to install or remove pkg
dnf update/upgrade
dnf grouplist		--> to list groups of pkgs
dnf groupinstall "grouppkg name"	--> install pkg in groups

ls /etc/yum.repos.d/		--> contains repos(dir with .repo ext) having online repo urls


---install any tool on linux using source code---
wget <tool tar ball url>
tar xvf <tool tar bal> 
cd <tool>
./configure  	--> to configure the tool according to env
make 	    	--> to compile tool
make install	--> to install compiled tool



Day68: Configure local YUM/DNF repository
dnf config-manager --add-repo=file:///localrepo/BaseOS/		--> configure repo locally . --add-repo=<path/url>

yum/dnf repolist all		--> to see repo list
--install epel repo---
epel : extra pkg enterprise linux
wget <epel repo url> 
dnf install <epel pkg>

createrepo	-->  used to create repo dir from scratch where repodata dir will create accordng to machine

----repo file contents----
[label]
name=
metadata_expire=-1	--> -1 means never
gpgcheck=1		--> to check all packages authenticity. to disable-0 
enabled=1		--> to enable repository. 0-disable
baseurl=file:///localrepo/BaseOs/		--> repo path or url
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release


----create repo manually-----
create local repo manullay
1. cd /etc/yum.repos.d
2. vi local.repo
[label] 	
name=BaseOS
metadata_expire=-1	--> -1 means never
gpgcheck=1		--> to check all packages authenticity. to disable-0 
enabled=1		--> to enable repository. 0-disable
baseurl=file:///localrepo/BaseOs/		--> repo path or url
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
3. dnf repolist all		--> shows local available repo

There are 3 ways to achieve:
1. use dnf config-manager --add-repo=<repo_url>  	--> which wil download the repoand add it under /etc/yum.repos.d/\
2. we can use createrep utility to create repo manually 
3. cd /etc/yum.repos.d/   create repo file with [label] name,enabled=1 and base_url



Day70: Network Management
interfaces:
ens160	: bridged network adaptor
lo	: loopback address specified in NIC card.
virbr0	: used to create further virtual network

Network modes:
Host-only	: VM will be assigned one IP. its accessible only on machine where vm running. not by other machine.
NAT		: just like home network with wirelss router. vm wil be assigned in separate subnet. 
bridged		: VM will be in the same network as your host. if your host IP is 172.16.120.45 then your vm wil be like 172.16.120.50. it can be accessed by all computers in host n/w.

mode		VM -> Host	Host-> VM	VM <-> VM	VM-> LAN/NET	NET/LAN --> VM
host-only	+		+		+		-		-		
bridged		+		+		+		+		+
NAT	 	+		Port forward	-		+		port forward

nmcli dev status		--> shows available n/w adaptor status
nmcli con show			--> shows connected adaptors

ip addr or ip a or ifconfig		--> to see the Ip details of n/w adaptors.

/etc/sysconfig/network-scripts	: contains adaptor files(added automatically when you add new adaptor connection). below are few files propeties
BOOTPROTO=dhcp/None
ONBOOT=YES

nmcli con add type ethernet con-name <con name> <adaptr name>		--> when you add new adaptor. we have to make it active by this command. IP will be assign by DHCP
nmcli connecton down/up <con name>  or ifdown/ifup <con name> 			--> to bring adaptor in connected/disconnect state 


nmcli con mod <con-name> connection.autoconnect yes/no				--> to auto connect or not. you can also update adaptor file and add ONBOOT=YES/NO 
nmcli con mod <con-name> connection.permissions user:<, separted username> 	--> to provide user permisison to modify nmcli settings.

nmcli con reload		--> reload all connections
nmcli con delete <con-name>	--> to remove adaptor device from machine


nmtui		--> also works like nmcli. however its graphical interface to make changes
open /etc/sysconfig/network-scripts/<adaptor files> 		--> to make adaptor changes.


------assign static IPv4/IPv6 address on machine-----------
nmcli add con type ethernet con-name <con-name> ifname <adaptor name> ip4 <IP> gw4 <gateway ip>	--> to assig IP statically to any adaptor

nmcli connection modify <con name> +ipv4.addresss <Ip> +ipv4.dns <dns>		--> to add multiple IP to existng conn  add (-ipv4) to remove 

hostnamectl set-hostname <new hostname>		--> to change hostname




--------NIC Teaming(Link aggregation)/Network bonding-------------
Link aggregation(combining more than 2 NIC cards) refers to verious methods of aggregating multiple network connections in parallel in order to increase throughput beyond what a single connnection could sustain. and to provide redundancy in case of link fail. 

NIC Teaming also called as load balancing failover(LBFO), bandwidth aggregation, traffic failover and so on.
Terminologies:
Teamd	: nic teaming daemon that uses the libteam library to communicate with team devices vie linux kernal.
Teamctl : utility allows users to control teamd instance.  You can check and change the port status as well as switch between active and backup states.
runner(mode)	: units of code written in JSON and used for implementation of NIC teaming. (exmaple of runner mode: round robin,load balancing,active backup, boradcast)

Note: we can have upto 32 NIC in one server.

1. add 2 NIC adaptor in machine
2. nmcli dev status				--> shows adaptors in machine
3. nmcli connection add type team con-name team0 ifname team0 config '{"runner":{"name":"activebackup"}}'		--> create NIC teaming 
4. nmcli con show			--> shows connections in system
5. nmcli connection modify team0 ipv4.addresses 192.168.227.440/24 ipv4.dns 8.8.8.8 connection.autoconnect yes ipv4.method manual	--> assigning IP to team
6. nmcli connection add type team-slave con-name team0-part1 ifname  ens224 master team0     --> adding 1st salve to NIC teaming 
7. nmcli connection add type team-slave con-name team0-part2 ifname  ens225 master team0	    --> adding 2nd slave to NIC teaming 
8. nmcli connection reload			--> reload added configurations
9. nmcli connection up team0/team0-part1/team0-part2


 
Day73: What is routing and Manage routing table 
routing(layer3) is the process of selecting path for traffic in network. 
1. static routing: manual entries. (disadvantage: admin should have topology knowladge)
2. default routing : send packets to next hop( no matter to which n/w packet belongs)
3. dynamic routing: adjustment of the route according to current route in routing table. (-ve: consume more bandwidth + less secure than static)

netstat -rn	or route -n	--> to see routing table
ip route add <ip/24> via <gateway/next hop ip> dev ens160		---> add new entry to rout table. use del to delete route entry

cd /etc/sysconfig/network-scripts
vi route-<adaptername>							---> to service reboot when you add entries to route table 
<destination ip> via <gateway Ip>						/proc/net/route		is the file responsible for routing table

nmcli connection reload
systemctl restart NetworkManager



Day74: Network Sniffing and ssh configuration 
The process of monitoring all the data packets passing through network connections is called sniffing. 
wireshark				--> utility used to monitor connections
tcpdump tcp port 22			--> to check packets passing over tcp port 22


rpm -qa | grep openssh			--> utility used for ssh connection
systemctl status sshd			--> sshd daemon should always be running to work ssh

cat /etc/hosts				--> If want to use server name instead of IP then put entry in /etc/hosts

ssh username@IP command			--> execute command on server without logging into it. 

--password less auth---
vi server.txt
server1
server2
server3

for i in 'cat server.txt'; do ssh root@$i ssh-copy-id -i ~/.ssh/id_rsa.pub root@$i done;

-------Welcome banner---------
figlet utility used to create banner text. ex: figlet DevOps
1. pre-login	: when we do ssh, banner shows before we put password and enter into server.
vi /etc/ssh/banner.txt
Welcome to Server1

vi /etc/ssh/sshd_config
uncomment banner line
banner <banner file path>		--> banner /etc/ssh/banner.txt 

systemctl restart sshd

2. post-login
edit /etc/motd	file for post login banner


----disable root login through ssh------
cat /etc/ssh/sshd_config
permitRootLogin  yes/no				--> make it No to disable root login to enhance security


-------change default ssh port--------- 
To avoid unathourised access or secure it from attacker.

1.  update port number in /etc/ssh/sshd_config file
port 5152
2. update ssh port entry in firewall
firewall-cmd --list-all				--> list firewall settings 
firewall-cmd --permanent --add-port=5152/tcp	--> add ssh port in firwall 
3. to relload firewall settings
firewal-cmd --reload
4. update se linux entries
semanage port -l | grep ssh				--> to see selinux ssh port 
cat /etc/ssh/sshd_config | grep semanage	--> to check selinux syntax to add port number
semanage port -a -t ssh_port_t -p tcp 5152 
5. systemctl restart sshd
6. test ssh with 5152 port 
ssh user@server -p 5152


---Allow/Deny seleted user/group to login via ssh------
allow	: if we put selected user then only that perticular user can login through ssh. other users(including root) will be denied. 
vi /etc/ssh/sshd_config
AllowUsers user1 user2			--> allow only user1 and user2
DenyUsers  user3 user3			--> selected deny

systemctl restart sshd

AllowGroups <grp name>			--> use group allow or deny if have to allow or deny multiple users.
DenyGroups  <grp name>	


-----Allow/Deny access from selected IP/Network through ssh--------
TCP wrappers	: works only in RHEL 7

/etc/hosts*	--> below 2 files are used to allow or deny access from perticular IP 
hosts.allow
hosts.deny

vi /etc/hosts.deny
<service>:<IP>
sshd:192.168.0.198		--> cannot ssh from this IP. [sshd 192.168.0.0/24 --> to deny whole network subnet]

systemctl restart sshd		--> restart denied service	

Note: If we denied whole network and we also wants to provide access to single machine from that network then put that entry in hosts.allow.
hosts.allow file overrides hosts.deny file entries

TCP wrappers does not work in RHEL8

----RHEL8 uses sshd_service to allow/deny access from IP---
vi /etc/ssh/sshd_config
DenyUsers *@192.164.0.198			--> deny all users frm this IP. [DenyUsers *@192.164.0.*]	--> deny all users from whole nmetwork

Note: DenyUsers entry overrides AlloUsers enrty in RHEL 8


--------Configure ssh login alert notification-----------
dnf install -y mailx sendmail			--> these 2 utilities are imp to send mail alerts
systemctl enable/start sendmail

cd <enter>					--> goto user home directory for which user we have to configure mail service 
vi .bashrc
echo ALERT - ROOT login on shell "$HOSTNAME" on `date` `who` | mail -s "Alert: ROOT login detected" <mailID>

source .bashrc
mail						--> to check mail inbox
tail /var/log/maillog				--> to see logs



Day82: Reset forgotten root password
Boot system and reach grub menu
1. select first option in grub menu and press e			[in RHEL9 select rescue option]
2. select line where linux has been written and go to end of line. replace rhgb quit with rd.break enforcing =0
3. ctl+x
4. mount -o remount,rw /sysroot		--> remounting sysroot from read only fs to rw mode. (single user mode)
5. chroot /sysroot
6. passwd 				--> chnage root password
7. touch /.autorelabel
8. exit					--> exit chroot env
9. exit and enter to boot system	--> exit maintenance mode 


rd.break enforcing=0 --> These parameters modify the behavior of the kernel and the initial ramdisk (initramfs) during the boot sequence.
rd.break : This parameter interrupts (pause) the boot process at an early stage and drops the system into emergency mode or a minimal shell 

rhgb:  parameter enables the graphical boot screen with the Red Hat logo and progress bar during the boot process. 
quite: suppresses most of the informational messages that are displayed during the boot process


-------Set or remove GRUB password------------
If you have forgotten the root password, But if you can see GRUB options then one can easly recover root password. 
what if attacker change root password if one get access to BOOT menu? so to increase security of system. 

1. it prevents access to single user mode
2. unauthorized access to system 
3. prevents access to GRUB console

----set GRUB password
1. cat /etc/grub.d/10_linux | grep gnu-linux
2. remove --unresticted from class= line
sed -i "/^CLASS=/s/ --unrestricted//" /etc/grub.d/10_linux 
3. ls /boot/grub2/				--> if we dont have grub password set then we wont see user.cfg file here
4. grub2-setpassword				--> set grub password
5. ls /boot/grub2/				--> can see user.cfg file
6. grub2-mkconfig -o /boot/grub2/grub.cfg	--> let kernal know grub changes


---remove grub password
1. add --unrestricted in  gnu-linux line in /etc/grub.d/10_linux file
2. remove user.cfg from /boot/grub2/
3. grub2-mkconfig -o /etc/grub.d/10_linux			--> make grub changes permanent

or 
vi /etc/grub2.cfg
comment for loop of password_pbk
shutdown -r now

Note: If you forgot both grub + root password then boot with iso image and do chrooting to reset grub password. 


Day84: Automate backup using rsync and cron
yum list installed rsync
dnf install -y rsync
rsync -v source/* destination/ --progress		--> to backup source to destination 
rsync -av -e ssh source/* root@IP:/destination/		--> -a archive -e remote shell	copy backup on remote machine
  


Day85: Firewall
Firewall is the process used to manage and monitor in-coming and out-poing traffic from network. 
	       RHEL6				RHEL7						RHEL8
		GUI			GUI			CLI			GUI		CLI	
	 	⌄			 ⌄			 ⌄			 ⌄		 ⌄
	 	⌄			 ⌄		 	 ⌄			 ⌄		 ⌄
	    system-config-firewall	firwall-config	     firewall-cmd	firewall-config    firewall-cmd (package)
	       ⌄					⌄					⌄
	       ⌄					⌄					⌄
	iptables(daemon service)		firewalld(service)			firewalld(service)
					⌄							⌄
					⌄							⌄
				     iptables						    NFTABLES(packet filter mechanism)
					⌄							⌄
					⌄							⌄
			   kernal(netfilter module)					kernal(netfilter module)

commenly used ports:
SSH	: 22
Telnet 	: 23
FTP/SFTP: 20/21
SMTP	: 25
DNS	: 53
HTTP(s)	: 80/443
DHCP	: 67/68
POP3	: 110

dnf install -y firewalld firewall-config
systemctl start/stop/enable firewalld

firewall-cmd --state			--> shows firewallcmd status
firewall-cmd --get-default-zone		--> get default zone being used 

firewall-cmd --list-all			--> shows firewall configuration 
firewall-cmd --get-services		--> shows default services firewall knows
firewall-cmd --zone=public(if deafult then not needed) --permanent --add-service=http or {http,nfs}(this service should be there in firewall services list)	--> add new service to firewall. --remove-service(for removal)

firewall-cmd --reload			--> reload firewall settings
firewall-cmd --zone=home  --change-interface=ens160	--> change the zone an interface belongs to 
firewall-cmd --get-active-zones		--> list active zones

ls /usr/lib/firewalld/		--> contains network scripts
ls /etc/firewalld/		--> contains all dir copies from /ur/lib/firewalld. firewall.conf --> file used to change firewall conf globally

cd /etc/firewalld/services/	
vi <service_name>.xml		--> to add new services to firewall-list but to add that services we need to run --add-srvice=<service_name> command

firewall-cmd --permanent --add-port=80/tcp	--> add service by port number. --remove-port(to remove port)
firewall-cmd --permanent --add-forward-port=port=80:proto=tcp:toport=8080 		--> port forwarding

firewall-cmd --permanent --add-masquerade	--> to enable masquerading servive
firewall-cmd --permanent --add-forward-port=port=443:proto=tcp:toport=443:toaddr=192.168.16.12		--> masqeurading(only work on ipv4)

Note: basic difference between iptables and firewall-cmd is you have to stop existing firewall rules before you add new one. otherwise firewall-cmd --reload will wipe out existing firewall rules and only accept newly added. 
but this is not the case in RHEL7/8

firewall-cmd --permanent --add-rich-rule 'rule family="ipv4" source address="192.168.16.16" port port=22 protocol=tcp reject'	--> reject traffic from IP

firewall-cmd --permanent --add-rich-rule 'rule family="ipv4" source address="192.168.16.16" service name=ssh or port port=23 limit value=1/m accept'	--> accept telnet only from perticular IP with limit connection per min.

firewall-cmd --permanent --add-rich-rule 'rule protocol value=icmp reject'		--> reject ping from all sources.

icmp --> network later protocol used by n/w devices to diagnose network connectivity issue. (ping or telnet) 
firewall-cmd --permanent --add-icmp-block=echo-request			--> to disable icmp acknowladgement. ping will not work 

firewall-cmd --enable-panic		--> block all network traffic in case of emergency
 

Note: Make sure to reload firewall-cmd after every firewall configuration.



Day87: Security enhanced linux (SELINUX)
added security layer works on mandatory access control(MAC) mechanism in linux kernal to make linux very secured and different from other os. 
selinux implement MAC mechanism(as per predefine rules. enforced or no)in kernal, checking for allowed permissions after std discretionary access control(DAC) are checked. by default enabled in fedora. 
SeLinux security = Role based access control(RBCA) + type enforcement(TE) + multilevel security(MLS)
terminologies in selinux:
objects		--> dir or files
subjects	--> process or user

total 3 modes of selinux:
1. enforced	: Selinux denies access based on policy defined. 
2. permissive	: does not deny access but denail access are logged in logs for actions that would have been denied if running in enfored mode.  
3. Disable	: Only DAC(user control permissions) rules are used. 


Note:  key difference between MAC and DAC lies in who has the authority to set access controls. In MAC, the control is centralized and determined by system policies, while in DAC, it is decentralized and left to the discretion of the resource owner. Many systems, including Linux, use a combination of both models to provide a balance between flexibility and security.

getenforce	--> used to check selinux mode
setenforce 0/1	--> 0-permissive	1-enforced

SELinux context(or label): SELinux context is a set of labels attached to processes and files, defining their permissions and roles in a security-enhanced Linux system. it contains additional info like selinux user,role,type and securty level etc. when running selinux all this info used to make acl decisions. 

SELinux user:role:type:level	--> syntax
user	: its user identity mapped to selinux roles to apply ACL
role	: attribute part of RBAC. domains --> roles --> selinux users.
type	: type enforcement decides domain for processes.(and where it is created if file)
level	: attribute of MLS (sensitivity(s0-s15):category-set(c0-c1023))

ls -Z filename			--> -z shows selinux settings
ls -dZ dirname			--> shows selinux settings on dir
ps -eZ pid			--> shows process selinux info
id -Z				--> user selinux info. by default unconfined
sestatus			--> shows selinux status
semanage login -l		--> shows local user mapping with selinux user info

tail /var/log/audit/audit.log		--> selinux logs(auditd.services runs at background) 

SELinux architechture: SELinux is linux security module built into linux kernal. it is driven by loadable policy rules. When security relevant access is taking place such as process attempts to access file,operation is intercepted in kernal by SELinux, if SELinux policy rule allows operation, it continues otherwise, operation is blocked.   

selinux policy:
1. Targeted policy: default selinux policy used in fedora. when used, processes that are targeted run in confined domain else run in unconfined domain.
subjects running in unconfined domain cannot allocate writable memory and execute it to avoid vulnerability attack. 

confined processes: almost all the processes that listen on network like sshd,ftp is confined in fedora. (avoid attacks) confine processes run in its own
domain. Even if attacker gets control of confined process then damage will be minimul bcz confined process cannot access other domains. 

unconfined processes: If unconfined process gets compromized then attacker can access any other process running in system.
processes started by systemd or which does not use network run in unconfined domain. 
unconfined_service_t	--> if process started by init
kernal_t		--> process started by kernal
unconfined_t		--> started by linux user

chcon -t <service type> <service/object>	--> change context temporarily. [chcon -t bin_t httpd] making httpd to run as unconfined process. 
restorecon service/object			--> restore context of server [restorecon httpd]


confined user: by default selinux users are unconfined. However we can confined them due to security restrictions. confined users are restricted by selinux
rules explicitely defined in current security policy. 
seinfo -u/-r		--> shows selinux users + roles. setools-console utility requires(audit selinux logs). 

useradd -Z selinux_role username	--> by default users are unconfined. but can make them confined using this command. 
semanage login -m -s user_u -r s0 __default__		--> modifying __default__ selinux user as confined. so whenever new local user created in linux it wil be confined under __default__ selinux user.
semanage login -m -s unconfined_u -r s0 __default__	--> make all new users unconfined. 

semanage boolean -l		--> check selinux user permissions bollean status. through this booleans only we can restrict permissions. 
setsebool -P ssh_sysadm_login on/off	--> provide ssh access to sysadmin if on(confined user) -P: permanently 


rpm -qi policycoreutils			--> utility required for basic basic operations of selinux systems
rpm -qi selinux-policy			--> this is base package for SELinux reference policy
rpm -qi selinux-policy-targeted 	--> SeLinux reference policy targeted base module
rpm -qi libselinux			--> provides api for selinux applications 
rpm -qi libselinux-utils		--> contains utilities (avcstat,getenforce,getsebool etc)
rpm -qi libselinux-python		--> required python bindings for selinux applications
rpm -qi selinux-policy-devel		--> provides custom selinux policy and policy modules.
rpm -qi selinux-policy-mls		--> to use multilevel security
rpm -qi settroubleshoot-server		--> used to troubleshoot selinux issues. 
rpm -qi policycoreutils-python		--> semanage,audit2allow,chcat etc
rpm -qi policycoreutils-gui		--> if want to manage selinux through gui

2. minimum	: modification of targeted policy. only selected processes are protected.
3. mls		: multi-level security protection. use sensitivity and category pair values to define confidentiality level. If we use mls then even root
can't have all the access. it can only access things defined in selinux for root user. 

cat /etc/selinux/config		--> file contains policy info and its modes. can use to change selinux mode permanently 

chcon -t <service type> <service/object>		--> temporaily change context of service or object
semanage fcontext -a -t samba_share_t /etc/file		--> make entry in selinux policy. but implement post restart
restorecon -v /etc/file					--> make changes permanently by implementing policy doc

semanage fcontext -a -t samba_share_t "/web(/.*)?" 	--> change selinux type of all files specified under dir. use -d to delete selinux context
restorecon -v /web/*					--> relabel selinux context. 

semanage port -l			--> list ports managed by selinux
semanage port -a -t ssh_port_t	-p tcp 5555	--> to add 5555 as ssh port in selinux. use -d instead of -a to delete rule in selinux
touch /.autorelabel			--> it reset selinux changes made though chcon and service reboot  

cp file /dir				--> change seliux context of file to seliux context of dir
cp --preserve=context file /dir		--> preserves selinux context of file even after copying under dir.


---enable selinux mls-----
1. dnf install -y settools-console
2. edit /etc/selinux/config
make mode=permissive 		--> from enforcing
3. touch "F" >>/.autorelabel	--> to reset selinux changes
4. reboot
5. getenforce to see the change 



Day88: SSH Port forwarding or tunneling 
SSH port forwarding allows you to forward otherwise insecure tcp traffic inside secure ssh tunnel from source to destination.(can be used for FTP,HTTP,SMTP,
TELNET etc)
provide encryption + authentication + must create new ssh for tunneling 
1. local port forwarding: create local port that is connected to remote service.
2. remote		: forward a port on remote server on internet to local port. executing command from remote machine for local 
3. dynamic 		: used to bypass firewall. used SOCKS protocol proxy port


-------local port forwarding between 2 servers---
webserver: http service running on port 80
local server: ssh -f -N -L localhost:5555:<webserver ip>:80 root@<webserverip> 		--> you can see port forwadring process running on local machine.

---local port forwarding with 3 servers-----
webserver: http service running on port 80
gateway server: 
local server: ssh -f -N -L localhost:5555:<webserver ip>:80 root@<gateway IP>

------local port forwarding with 3 servers using gateway port-----
webserver: http service running on port 80
gateway server: firewall-cmd --permanent --add-port=5555/tcp  + firewall-cmd --reload
local server: ssh -g -f -N :5555:<webserver ip>:80 root@<webserver ip>


--Remote port forwarding between 2 servers---
webserver: http service running on port 80
webserver: ssh -f -N -R localhost:5555:<webserver ip>:80 root@<local server IP>


----Dynamic port forwarding-----
webserver: http service running on port 80 buts blocked by firewall
local machine: ssh -f -N -D localhost:5555:<webserver IP>:80 root@<webserver IP>
	       curl --proxy socks5h://localhost:5555 http://<webserver IP>:80 


Day89:
-----------Shell Scripting-----------------
cat /etc/shells or chsh	-l 		--> shows shell list
chsh	--> enter shell to be used	--> change default shell
echo ''		--> print as it is
echo ""		--> print value 
echo -e "HEll\nShantayya"		--> use -e if want to use \
#!/bin/bash				--> shebang line tells interpreter that which shell we are using 

# 	: comment
$	: to print value of variable
export var=		--> to globally define the variable

----read operator---
read name			--> to take single input from user
read name1 name2 name3 name3	--> take multiple inputs from user
read -p 'Enter user Name' name	--> input cursor stays in same line 
read -sp 'Enter password' pass	--> take silent input

echo "Enter user names"
read -a  names	--> take multiple inputs(space separated) as array

read 
echo "Entered name is $REPLY"		--> If we dont provide any variable with read then value stored in REPLY variable. 

---Passing the argument in script----
system reserve variables used temporarily to hold value ($1,$2,$3 etc)

vi script.sh 
#!/bin/bash
echo $1 $2 $3 $4

./script.sh hi how are you?

instead of system reserve variabes we can use args array: 
args=("$@")
echo ${args[0]} ${args[1]} ${args[2]} ${args[3]}	or echo $@			--> it will print 4 arguments 

echo $#				--> shows number of arguments passed

---Coditional statement-----
string comparison operator:
[ "$a" == "$b" ]
[ "$a" != "$b" ]
[[ "$a" < "$b" ]] use [[ ]] for <,>,<=,>= string operator.

integer comparsion operator:
[ "$a" -eq/ne/lt/le/gt/ge "$b" ]
("$a" </> "$b")
(( "$a" <= "$b" )) or (( "$a" >= "$b" ))

echo -e "Enter name of file: \c" 	--> -e used in echo to use \ char and -c to keep cursor at same place. 
read num1 num2
if [ $num1 -eq $num2 ]
then
 echo "Num1 is equa t num2"
elif [ $num1 -gt $num2 ]
then
  echo "Num1 greater than num2" 
else
 echo "Num1 less than num2"
fi

----File test operator-------
[ -e $filename ]	--> -e means exists or not. -f means regular ile or not. -d means directory or not, -b means block file or not, -c char file or not. -s empty file or not. 
-r file has read perm or not, -w file has write perm or not, -x file has execute perm or not. 

----Nested If condition-----
echo -e "Enter file name: \c"
read filename
if [ -f $filename ]
then
	if [ -w $filename ]
	then	
		echo "Please enter text here and press ctl + d to exit"
		cat >> $filename
	else
		echo "$filename does not have write access"
	fi
else
	echo "$filename does not exist"
fi


----Logical AND operator-----&&, ||
echo -e "Enter your age: \c"
read age
if [ "$age" -gt 18 ] && [ "$age" -lt 40 ]
then
	echo "Age is valid"
else
	echo "Age is not valid"
fi

echo -e "Enter your age: \c"
read age
if [ "$age" -le 18 ] || [ "$age" -ge 40 ]
then
	echo "Age is valid"
else
	echo "Age is not valid"
fi


-----Arithmatic operator on integers-----
$((operation)) or $( expr $num1 operator $num2 )  --> to use * with expr use \* [ $( expr $num1 \* $num2 )]
echo "sum is $((2+3))"

----Arithmatic operation on decimal numbers---
bc	--> basic calculator. this utility is required to execute bc command.
echo "2/3" | bc
echo "scale=2;2/3" | bc		--> use scale=number for decimal places
echo "scale=2;sqrt(10)" | bc -l	--> use -l(library) with bc for advanced operation.


Day119: AWK command
awk is mostly used for pattern matching and processing + data transformation. 
syntax:
awk options 'selection_criteria {action}' input_file > output_file

vi employee.txt
Shantayya  Manager  Accounts  50000
Shankar    Manager  Sales     100000
Somnath    Master   Operation 500000
Vir        Head	    Account   1500000

awk '{print}' filename		--> print file details like cat
awk '/Manager/ {print}' employee.txt	--> print nly Managers
awk '{print $1 $4}' employee.txt	--> print 1st and 4th col. or use NF(number of fields) [awk '{print $1,$NF}' employee.txt]
awk '{print NR,$0}' employee.txt	--> use NR to show record numbers $0 for all records
awk 'NR==3, NR==4 {print NR,$0}' employee.txt	--> record 3 to 4
awk 'END {print NR}' employee.txt	--> print last record number. to see hw many lines are there in file. 

use for loop in awk
awk 'BEGIN { for(i=1;i<=6;i++) print "Square of" i, "is", i*i; }' 

awk -F ":" '{print $1}'	/etc/passwd		--> default separator is space in awk.we can use diff separator using -F field.
awk -F "\" '/^\// {print $NF}' /etc/shells	--> print shell names only
awk 'length($0) < 8' /etc/shells		--> print records length less than 8


----Pattern Matching-----------
<	: matching begining of word
>	: matching end of word
[:alnum:] or [a-zA-Z0-9]
[:digit:] or [0-9]
[:punct:] 
{m}	: matching exact count	{m,}	: matching atleast m count	{m,n}	: matching preceding regex m to n times
cat input | grep -o [0-9]* 	--> -o means only matching pattern


------------RHEL8(cloud ready) -----------------------
Cockpit: web based server management tool   --> its not a service rather socket service which works 2 way. Its very powerful tool to manage linux machine from anywhere. 
dnf install -y cockpit
systemctl enable cockpit.socket --now		--> start abd enable cockpit socket
firewall-cmd --list-all				--> to check cockpit service is added in firewall 
IP:9090						--> to access cockpit server

Note: We can also monitor multiple hosts. you just have to ad host in cockpit. Cockpit is RHEL8 exclusive feature. 

---------------KVM-------------------------
kernal based virtual machine  --> utility through which we can convert machine into hypervisor. 
grep -e 'vmx' or 'svm' /proc/cpuinfo			--> to cheeck machine supports virtualization or not. 

----------Using image builder(composer)--------
from the package we can create ready to deploy images and deploy anywhere. 

---------system tuning profiles----------
tuned ---> is daemon service used to perform auto tuning of linux machine. it monitor the performance of machine and suggest best tuning profile. 
1. static tuning	--> tuned applies kernal parameters as per the tuned profile selected. 
2. dynamic tuning	--> Continuosly monitor system and changes system settings as per work load. 

dnf install -y tuned			--> tuned already installed if os installed with server with GUI mode. 
ls /etc/tuned				--> shows tuned configuration files. usually we dont change directly.
ls /usr/lib/tuned			--> shows tuned profile files 
systemctl enable --now tuned		--> start tuned service
tuned-adm active			--> used to see active tuned profile
tuned-adm list				--> list of tuned prfiles
tuned-adm profile <profile-name>	--> like balanced,virtual-guest,accelerator-performance etc
tuned-adm recommend			--> to see tuned profile suggestion 
tuned-adm off				--> to disable tuned profile. if we execute tuned-adm active --> it will say no active profile currently.   

we can create custom tuned profile.
1. create profle name directory under /usr/lib/tuned/
2. mkdir test-performance
3. cd test-performance 
4. vi tuned.conf	--> add
[main]
include=latency-performance
summary=this wil increase performance. 
5. tuned-adm list		--> will show new custome tuned profile. 



------configure NFS---------------
dnf install -y nfs*				--> utility required to configure nfs(nfs-utils,nfs4-acl-tools etc)
systemctl enable --now nfs-server.service 	--> to start and enable nfs service. 
netstat LISTEN | grep 2049			--> to check nfs port opened or not

create nfs 
1. mkdir /mnt/nfs_share/doc
2. chown -R nobody: /mnt/nfs_share/doc		--> nobody is normal user. not prilivaged one. so purpose of nobody to allow anyone to log inot system. default login shell /sbin/nologin
3. chomod -R 777 /mnt/nfs_share/doc		--> providing full permision as every user can create file
4. systemctl restart nfs-utils.service
5. vi /etc/exports				--> configure nfs
/mnt/nfs-share/doc	client IP/subnet-mask(rw,sync,no_all_squash,root_squash)

sync		: write data to disk first and then reflect
no_all_squash	: same username and groupname will be used. if username not present in client or server then ID will show
root_squash	: root can't have separate privilage. even if root change something then username and groupname will by nobody only. (no_root_squash --> root can do anything)
6. exportfs -arvf	--> to implement nfs configuration
7. firewall-cmd --permanent --add-service=nfs		--> add nfs service in firewall
8. firewall-cmd --permanent --add-service=rpc-bind + mountd	--> rpc-bind works as supportive nfs service. it redirects client to correct server port. mountd used to mount nfs at client server locally 
9. firewall-cmd --reload

--client side
mkdir /mnt/nfs_share
mount -t nfs nfsIP:/mnt/nfs_share/doc /mnt/nfs_share
df -hT


AutoFS	--> used for on-demand mounting of nfs
follow same steps untl 9
client side:
1. dnf install nfs-utils nfs4-acl-tools,autofs -y
2. mkdir -R /automount/private /automoubt/public		--> on nfs server(vi /etc/exports				
/public		client IP/subnet-mask(ro,sync)
/private	clientIP/network(rw,sync)
3. vi /etc/auto.master
/automount(mount point on client machine)	/etc/automount.txt (file having nfs mounting details)	--timeout=30(sec)
4. vi /etc/automount.txt
public	-ro,sync	nfs-ip:/public
private	-rw,sync	nfs-ip:/private
5. systemctl enable --now autofs
6. df -hT				--> it will not show you nfs mount point. once you cd /automount	then it will show mount points	

ACL on nfs --> iin nfs2,nfs3 acl was not allowed however acl can be applied on nfs4. normal acl commands does not work on nfs acl. 
we use ACE(access control entries)
[access type]:[flags]:[principal]:[permissions]
1. access(A) or deny(D)
2. D: inheriting permision on directory, F: file,
3. entity like user or group
4. permissions r,w,x etc
 
Note: to use acl on nfs we must specifiy do user,roup mapping through no_all_squash in automount.txt file at server side. 
nfs4_getfacl <filename>
nfs4_setfacl -a A::1000(uid):RWX cal.txt	or nfs4_setfacl -e filename	--> to edit acl permissions 


----------Nginx----------------
web server can be used as reverse proxy(server side),load balancer,http cache and mail proxy etc
1. create nginx repo
[nginx]
name: nginx package
baseurl:https://nginx.org/packages/centos/8<os version>/$basearch
gpgcheck=0
enabled=1

sudo yum undate
sudo yum install -y nginx
sudo systemctl enable --now nginx

/etc/nginx/nginx.conf			--> nginx configuration file
/usr/share/nginx/html			--> pages from this location wil get served when requested. 
/var/log/nginx				--> logs related to nginx

firewall-cmd --permanent --add-service={http,https}
firewall-cmd --reload

Note: proxy server can be categories as forward proxy(client side) and reverse proxy(server side) 
------configure nginx as proxy:
vi nginx.conf
http--> server--> 
location /{
 # proxy_pass <destination serverIP:port
   proxy_pass http://127.0.0.1:8080;
}
systemctl restart nginx
getsebool -a | grep httpd		--> get https related selinux configurations
setsebool httpd_can_network_connect on -P	--> enable selinux httpd connectvity 


---configure nginx as load balancer:
vi nginx.conf
http--> server--> 
upstream notes<lb name>{			--> add this directory
	server server1:7010
        server server2:7010
        server server3:7010
 }
server{
 listen 80;
location /{
 # proxy_pass <destination lb name>
   proxy_pass http://notes;
}
}
sudo nginx -t		--> to check any typos in nginx.conf


----------Linux Patching-------------
patches are new lines of code released to fix the existing bugs,introduce new features and improve performance etc
Standard patching procedure:
1. check for patch updates i.e dnf check-update;dnf repolist all
2. check for system downtime with teams
3. raise change request through serviceNow 
4. collect pre-check infor and document them 

#!/bin/bash

echo -e "Date information" >>/home/swami/"pre_patch_$(date+"%d-%m-%Y").txt"
date >>/home/swami/"pre_patch_$(date+"%d-%m-%y").txt" 2>&1

echo -e "Mounted file system" ----
df -hT >>

echo -e "Block ID infor" --
blkid >>
lsblk >>
fdisk -l >>
cat /etc/fstab >>
vdgisplay >>
lvdisplay >>
multipath -ll >>
ifconfig -a >>
free -m >>
uptime >>
cat /etc/grub2.cfg >>
ps -ef >>
top >>
route -n >>

5. take server backup or snapshot
6. co-ordinate with other involed teams to patch.
7. execute above script as post-check and compare results
8. check with teams that apps/d working as expected
9. close change

manually patching:
dnf update <package-name> -y 
dnf downgrade <package-name> -y		--> if want to rollback update for perticular package
dnf history 				--> shows dnf event

dnf history info <ID> 			--> to see detailed info about event you will find ID in dnf history command
dnf history undo <ID>			--> to rollback dnf event

dnf update -y				--. update all packages
reboot


ansible patching:
?
- name: Patching through ansible
  hosts: Middleware
  become: true
  tasks:
   - name: execute pre-checks
     script: /home/swami/precheck.sh
     args:
      - creates: /home/swami/precheck.sh
   - name: update packages
     dnf: 
       name: "*"
       latest: yes
     notify: Reboot machines

   - name: execute post checks
     script: /home/swami/postcheck.sh
     args: 
       - creates: /home/swami/postcheck.sh
   - name: get result file names
     shell: find /home/swami/ -type f -name "*.txt" | cut -d '/' -f2
     register: result
   - name: fetch checks results
     fetch: 
       src: /home/swami/{{ item }}
       dest: /home/swami/
     with_items: "{{ result.stdout_lines }}"
  handlers:   
   - name: reboot machines
     reboot: 
       reboot_timeut: 60



----www.googgle.com-----How it works
1. Browser Cache: The browser first checks its cache to see if it already knows the IP address of www.google.com from a previous visit. If it finds it, it can skip the DNS resolution step.

2. /etc/hosts File: The browser may also look in the computer's local /etc/hosts file to see if there's a manual entry for www.google.com and its corresponding IP address.

3. Local DNS: If the IP address is not in the cache and not in the /etc/hosts file, the browser checks with the local DNS resolver. This resolver can be on your computer or on your network, like your router. If it knows the IP address(maintains cache), it provides it; otherwise, it forwards the request to the next step.

4. Root Name Server (RNS): If the local DNS doesn't know the IP address, it queries the root name server. The root name server helps identify the top-level domain (TLD) for www.google.com (in this case, the TLD is ".com").

5. Top-Level Domain Name Server (TLD NS): The RNS directs the request to the appropriate TLD NS for ".com." The TLD NS can provide information about which authoritative name server (NS) is responsible for the "google.com" domain.

6. Authoritative Name Server (Google's NS): The TLD NS points to Google's authoritative name server, which has the exact IP address for www.google.com.

7. Connecting to Google's Load Balancer: With the IP address in hand, the browser connects to Google's load balancer. Load balancers distribute incoming requests to multiple web servers to balance the load. They use algorithms like Round Robin, Least Connection, Least Time, Hash, IP Hash etc., to determine which web server should handle the request.

8. Web Server Response: The load balancer forwards the request to one of Google's web servers. The web server processes the request, which might involve running various programs, accessing databases, etc., to generate the Google homepage.

9. Firewall and Load Balancer: The web server sends the response back to the user's browser through any necessary security measures, such as firewalls and load balancers again.

10. Browser Rendering: The user's browser receives the response, interprets it, and displays the Google homepage on the user's screen.

URL: protocol://hostname:port

protocl sub-domain 	 TLD	    path	query string separator	
  ↓	↓		  ↓	      ↓            ↓
https://www.nehraclasses.in:443/blog/article/search?docid=720&hl=en#daytwo
		↓	     ↓				↓		↓
		domain	    port(optional)		query		fragment(part of page)



--------------TCP/IP layer-----------------

Application Layer(Data)       --> data is converted into packets(HTTP) 									     Application Layer
	↓																	     ↑	
Transport Layer(Segment)     --> Organize the data packets and deliver with help of IP(TCP)							Transport Layer
	↓																	     ↑
Internet Layer(Packet)		--> Identify the receiver (IP)											Internet Layer
	↓																	     ↑
Network Layer(Bits)		--> hardware and ethernet. find the correct machine with IP over network and pass packets to correct machin. 	Network Layer
        ↑																	     ↑		
      Sender							<------ Receiver send acknowledgement	<-----					   Receiver


Note: TCP will not work without IP. UDP is a connection less protocol and does not require IP to deliver messeges as it broadcast packets over network. 
packets may loss in middle as sender dont get any ackowlegement from receiver. 


-------------OSI model-----------
open system interconnection means to connect two devices with each other. advanced layer of TCP/IP model 

Application Layer    -------------
Presentatin layer		  |  --> Software layer	
Session Layer        -------------
Transport Layer                    ---> connects s/w with h/w layer(heart of OSI)
Network Layer	     ------------- 
Data Link layer			 | --> Hardware layer
Physical layer       -------------


Application Layer: provides user interface (all browsers + uses protocols like http,smtp,ftp etc)
Presentation layer: translates data into character encoding schemes + compress data + encryption of data
session layer: creates,manage and maintains session until user logout + handles error using check points in data transfer
Transportation layer: service to service layer responsible to control data transfer + devides data into segments + makes necessary synchronizaton if sender and receiver speed differs. 
Network Layer: converts segments into packets + selects quick path using routers
Data Link Layer: converts packets into frames + switches and hubs operate under this layer + MAC address used to tranfer data from one system to other 
Physical layer: converts frames into bits + if using ethernet cable --> tranfer data in electric signals, wifi--> tranfer data in radio frequency + decides unidirecti



----------------DNS-----------------------------------------
Domain name system or dns is tcp/ip service on network enabled client to translate names into IP address. 
nslookup www.google.com		--> forward lookup query
nslookup 8.8.8.8		--> reverse lookup query

Note: client computer need to know dns server IP which is provided by DHCP or manually entered in /etc/resolve.conf file 

DNS namespace: is hierarchycal tree data structure with root at top and represent . --> top level domain name(.com/.org/.in).
FQDN: fully qualified domain name is combination of hostname of machine appended with its domain name. 

A record: also called as host record which contains the (single) IPv4 address of machine. (AAAA record-> IPv6) . provide name --> gives IP
PTR record: its reverse of A record. used to resolve IP to hostname. (provide IP --> gives name)
SOA record: contains meta info about zone(contains domain name and its child domain names)
CNAME record: maps hostname to hostname. used to create alias for existing hostnames. (nick name like test.shantayya.in --> learn.shantayya.in)  

hostnamectl set-hostname <name>			--> statically assign hostname to machine
dnf install bind bind-utils -y			--> dns packages used on linux machine to configure dns
systemctl enable named --now			--> start and enable named service

/etc/named.conf					--> dns related conf

Time To Live (TTL) in DNS:
In the context of Domain Name System (DNS), TTL refers to the amount of time a DNS record is considered valid. It is a value set in the DNS record to indicate how long a resolver or a caching server should consider the information as authoritative before it needs to be refreshed. Once the TTL expires, the resolver needs to fetch the updated information from the authoritative DNS server.
In simple terms it represents how quickly a system can switch over from one server to another in the event of a failure.

--------------------------------IP Address---------------------------
IP Address = Network + Host bytes. 4 octets(1 octet =8 bits=1byte) where each octet is separated by .
ICANN assigns networks addesses to ISP. IP Address are categories into following types
Class A: 
Network (8 bits) + Host (24 bits) 
rule: 1st bit of the 1st byte is 0
range: 1-126
No. N/w: 2^(no of network - reserved no)= 2^(8-1) = 2^7= 128-2(1st-network addr and last-broadcast addr)
No.Host: 2^24

Note: 127 is loopback address

Class B:
Network (16 bits) + Host (16 bits) 
rule: 1st bit of the 1st byte is 1
      2nd bit of 1st byte is 0
range: 128-191
No. N/w: 2^(no of network - reserved no)= 2^(16-2) = 2^14 = 16384
No.Host: 2^16 = 65536-2 = 65534

Class C: 
Network (24 bits) + Host (8 bits) 
rule: 1st bit of the 1st byte is 1
      2nd bit of 1st byte is 1
      3rd bit of 1st byte is 0
range: 192-223
No. N/w: 2^(no of network - reserved no)= 2^(24-3) = 2^21 = 2097152
No.Host: 2^8 = 256-2 = 254

Class D: 224-239 Reserved for multicasting (one to many) above classes are unicasting
Class E: 240-255 Reserved for R&D department of USA

Understanding network and host portions is crucial for routing and addressing in networking. The division between these two portions is determined by the class of the IP address or, in the case of Classless Inter-Domain Routing (CIDR), by the subnet mask.
The network portion helps routers direct traffic to the appropriate destination network, while the host portion identifies specific devices within that network.

In the CIDR notation 192.168.1.0/24, the first 24 bits represent the network portion, and the remaining 8 bits represent the host portion. This allows for more flexibility in defining network sizes.




--------------------Real time use of Shell scripts----------------------
set -x #debug mode
set -e #Exit on error
set -o #Exit on pipefail

1. Functions: Organizing and Reusing Code
#!/bin/bash
# Define a function
greet() {
 local name=$1
 echo “Hello, $name!”
}
# Call the function
greet “Alice”
greet “Bob”

In this example, `greet` is a function that takes one argument (`name`) and prints a greeting message. Using `local` ensures the variable scope is limited to the function, preventing unexpected behaviour in larger scripts.


2. Error Handling: Making Scripts Robust
Error handling is crucial for creating reliable scripts. Using `set -e` ensures the script exits immediately if any command fails:

#!/bin/bash
set -e
mkdir /some/directory
cd /some/directory
touch file.txt
echo “All commands executed successfully!”

For more granular control, use `trap` to catch errors and execute a specific function:
#!/bin/bash
trap ‘echo “An error occurred. Exiting…”; exit 1;’ ERR
echo “Executing command…”
false # This command will fail
echo “This line will not be executed.”

In this script, if any command fails, the `trap` command will print an error message and exit the script.

3. Debugging: Finding and Fixing Issues
Debugging shell scripts can be tricky, but tools like `set -x` can help by printing each command before it is executed:

#!/bin/bash
set -x
echo “This is a test script.”
false # Intentional failure to demonstrate debugging
echo “This will not be printed.”

By running this script, you’ll see each command printed to the terminal, making it easier to identify where things go wrong.

4. Working with Arrays: Managing Complex Data
Arrays allow you to store multiple values in a single variable, which is useful for handling lists and other complex data structures:

#!/bin/bash
# Declare an array
fruits=(“Apple” “Banana” “Cherry”)
# Access array elements
echo “First fruit: ${fruits[0]}”
# Loop through the array
for fruit in “${fruits[@]}”; do
 echo “Fruit: $fruit”
done

This script demonstrates how to declare an array, access individual elements, and iterate through all elements.

5. Advanced Text Processing: Using `awk` and `sed`
`awk` and `sed` are powerful text processing tools that can manipulate text files and streams efficiently:

#!/bin/bash
# Using awk to print specific columns
echo -e “Name\tAge\nAlice\t30\nBob\t25” | awk ‘{print $1}’
# Using sed to replace text
echo “Hello World” | sed ‘s/World/Shell/’

In this example, `awk` prints the first column from a tab-separated list, and `sed` replaces “World” with “Shell” in the input string.

6. Script Parameters: Enhancing Flexibility
Scripts can accept parameters to increase their flexibility and usability:

#!/bin/bash
if [ $# -lt 2 ]; then
 echo “Usage: $0 <name> <age>”
 exit 1
fi
name=$1
age=$2
echo “Name: $name, Age: $age”

This script checks if at least two parameters are provided and then uses them within the script.

7. Integrating with Other Tools: Combining Power
Advanced scripts often integrate with other command-line tools and utilities. Here’s an example of using `curl` to fetch data from an API and `jq` to parse JSON:

#!/bin/bash
# Fetch weather data for a given city
city=”London”
api_key=”your_api_key_here”
response=$(curl -s “http://api.openweathermap.org/data/2.5/weather?q=$city&appid=$api_key")
# Parse and display the temperature
temp=$(echo $response | jq ‘.main.temp’)
echo “The temperature in $city is $temp Kelvin.”

This script uses `curl` to retrieve weather data and `jq` to extract the temperature from the JSON response.
Note: jq: A lightweight and flexible command-line JSON processor. It is commonly used to parse, filter, and manipulate JSON data in shell scripts or command-line environments. With “jq,” one can easily extract specific fields from JSON objects, and format JSON output according to your needs.

8. Background Jobs: Running Tasks Concurrently
Running tasks in the background allows scripts to perform multiple operations simultaneously:

#!/bin/bash
# Run tasks in the background
sleep 5 &
pid1=$!
sleep 10 &
pid2=$!					#store pid of bgrd jobs
# Wait for all background jobs to complete
wait $pid1
wait $pid2
echo “All background tasks completed.”

This script runs two `sleep` commands in the background and waits for both to complete before proceeding.

9. Automating Backup of a Directory
This script creates a compressed backup of a directory and stores it with a timestamp.
#!/bin/bash

# Variables
SOURCE_DIR="/path/to/source"
BACKUP_DIR="/path/to/backup"
TIMESTAMP=$(date +"%Y%m%d%H%M%S")
BACKUP_FILE="backup-$TIMESTAMP.tar.gz"
# Create a backup
tar -czvf $BACKUP_DIR/$BACKUP_FILE $SOURCE_DIR
# Output message
echo "Backup completed: $BACKUP_DIR/$BACKUP_FILE"

10. Monitoring Disk Usage
This script checks disk usage and sends an email if usage exceeds a certain threshold.
#!/bin/bash

# Variables
THRESHOLD=80
EMAIL="admin@example.com"
CURRENT_USAGE=$(df / | grep / | awk '{ print $5 }' | sed 's/%//g')
# Check if usage exceeds threshold
if [ $CURRENT_USAGE -gt $THRESHOLD ]; then
  echo "Disk usage is at $CURRENT_USAGE% on $(hostname)" | mail -s "Disk Space Alert" $EMAIL
fi


11. Monitoring CPU and Memory Usage
#!/bin/bash

# Variables
LOG_FILE="/var/log/resource-usage.log"
THRESHOLD_CPU=80
THRESHOLD_MEM=80
# Get CPU and memory usage
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2 + $4}')
MEM_USAGE=$(free | grep Mem | awk '{print $3/$2 * 100.0}')
# Log the usage
echo "$(date): CPU: $CPU_USAGE%, Memory: $MEM_USAGE%" >> $LOG_FILE
# Alert if usage is above threshold
if (( $(echo "$CPU_USAGE > $THRESHOLD_CPU" | bc -l) )) || (( $(echo "$MEM_USAGE > $THRESHOLD_MEM" | bc -l) )); then
  echo "$(date): High resource usage detected. CPU: $CPU_USAGE%, Memory: $MEM_USAGE%" | mail -s "Resource Usage Alert" admin@example.com
fi


12 Automating Docker Container Cleanup
This script removes all stopped Docker containers, dangling images, and unused networks.
#!/bin/bash

# Remove all stopped containers
docker container prune -f
# Remove all dangling images
docker image prune -f
# Remove all unused networks
docker network prune -f
# Output message
echo "Docker cleanup completed."

13  System Update and Upgrade
This script updates and upgrades the system packages on a Debian/Ubuntu system.
#!/bin/bash

# Update package list
sudo yum update
# Upgrade packages
sudo yum upgrade -y
# Clean up
sudo yum autoremove -y
sudo yum clean
# Output message
echo "System update and upgrade completed."

14 Checking Service Status and Restarting if Down
#!/bin/bash

# Variables
SERVICE_NAME="your-service-name"
# Check if the service is running
if ! systemctl is-active --quiet $SERVICE_NAME; then
  # Restart the service if it's not running
  systemctl restart $SERVICE_NAME
  echo "$SERVICE_NAME was down and has been restarted."
else
  echo "$SERVICE_NAME is running."
fi

15 Automating Log Rotation
This script rotates logs for a specified directory, keeping only a certain number of backups.
#!/bin/bash

# Variables
LOG_DIR="/path/to/logs"
MAX_BACKUPS=7
# Rotate logs
find $LOG_DIR -type f -name "*.log" | while read LOG_FILE; do
  # Rotate logs and delete old backups
  mv $LOG_FILE $LOG_FILE.$(date +"%Y%m%d%H%M%S")
  ls -t $LOG_FILE.* | sed -e "1,${MAX_BACKUPS}d" | xargs -d '\n' rm -f
done
# Output message
echo "Log rotation completed."


16 Automated User Account Creation
#!/bin/bash

# Variables
USER_LIST="/path/to/userlist.txt"
DEFAULT_PASSWORD="changeme"
# Loop through the list of users
while IFS= read -r USERNAME; do
  # Create user account
  useradd -m $USERNAME
  echo $USERNAME:$DEFAULT_PASSWORD | chpasswd
  echo "User $USERNAME created with default password."
done < $USER_LIST


17 Automated File Syncing Between Servers
#!/bin/bash

# Variables
SOURCE_DIR="/path/to/source"
DEST_SERVER="user@remote-server"
DEST_DIR="/path/to/destination"
LOG_FILE="/var/log/rsync.log"
# Sync files
rsync -avz --delete $SOURCE_DIR $DEST_SERVER:$DEST_DIR >> $LOG_FILE 2>&1
# Output message
echo "File sync completed. Check $LOG_FILE for details."

18. Database Health Check
#!/bin/bash

# Variables
DB_HOST="localhost"
DB_USER="yourusername"
DB_PASS="yourpassword"
DB_NAME="yourdbname"
# Check database connection
if mysqladmin ping -h $DB_HOST -u $DB_USER -p$DB_PASS --silent; then
  echo "Database $DB_NAME is up and running."
else
  echo "Database $DB_NAME is down."
  # Restart the database service (optional)
  # systemctl restart mysql
fi

19. Automating Cleanup of Old Files
#!/bin/bash

# Variables
TARGET_DIR="/path/to/directory"
DAYS_OLD=30
# Find and delete files older than the specified number of days
find $TARGET_DIR -type f -mtime +$DAYS_OLD -exec rm -f {} \;
# Output message
echo "Cleanup completed. Files older than $DAYS_OLD days have been deleted."

